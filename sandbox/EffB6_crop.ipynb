{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3afd98bf-5c0a-4e86-a250-9320d12a4283",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import albumentations\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom as dicom\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import torch\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from numba import cuda\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.augment import Aug, Aug_Crop, Aug_No_transform, Flip_Aug\n",
    "from src.generator import Generator, GetModel\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b9ef9c8-4057-41f5-8f88-04980d3eb57f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EFFB7': {'IMG_SIZE': 600,\n",
       "  'BATCH_SIZE': 4,\n",
       "  'SEED': 42,\n",
       "  'WEIGHTS': '/app/_data/noisy-student-efficientnet-b7/efficientnetb7_notop.h5'},\n",
       " 'CLASS': {'negative': 0, 'typical': 1, 'indeterminate': 2, 'atypical': 3},\n",
       " 'EFFB4': {'IMG_SIZE': 380,\n",
       "  'BATCH_SIZE': 50,\n",
       "  'SEED': 42,\n",
       "  'WEIGHTS': '/app/_data/efficientnet-b4_noisy-student_notop.h5'},\n",
       " 'EFFB0': {'IMG_SIZE': 224,\n",
       "  'BATCH_SIZE': 120,\n",
       "  'SEED': 42,\n",
       "  'WEIGHTS': 'imagenet'},\n",
       " 'EFFB6': {'IMG_SIZE': 528,\n",
       "  'BATCH_SIZE': 8,\n",
       "  'SEED': 42,\n",
       "  'WEIGHTS': '/app/_data/noisy-student-efficientnet-b6/efficientnetb6_notop.h5'}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/app/_data/predicted_crop_v1.csv\")\n",
    "list_wrong = df[(df[\"class\"] != \"negative\") & (df[\"label\"] == \"none 1 0 0 1 1\")][\n",
    "    \"id_image\"\n",
    "].tolist()\n",
    "df = df.query(\"id_image not in @list_wrong\").reset_index(drop=True)\n",
    "\n",
    "with open(\"/app/_data/base_config.json\", \"r\") as f:\n",
    "    base_config = json.load(f)\n",
    "base_config[\"EFFB7\"][\"SEED\"] = 42\n",
    "base_config\n",
    "MOD='EFFB6'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eea90a8-a4e0-4d0d-a91a-0fbd49ce44a7",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4340234d-f37b-4c32-800a-20a9aeb87198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy = keras.mixed_precision.experimental.Policy(\"mixed_float16\")\n",
    "# keras.mixed_precision.experimental.set_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9dae3d5-47e0-4c94-a91b-5485aefb72e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(\n",
    "    n_splits=5, random_state=base_config[MOD][\"SEED\"], shuffle=True\n",
    ")\n",
    "train_ids = []\n",
    "val_ids = []\n",
    "for train_index, valid_index in skf.split(df, df[\"class\"]):\n",
    "    train_ids.append(train_index)\n",
    "    val_ids.append(valid_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "608ae6bf-6177-4773-872c-7eafc5a866af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " epoch #0\n",
      "\n",
      "Warning! Model path already exists.\n",
      "Epoch 1/50\n",
      "603/603 [==============================] - 537s 841ms/step - loss: 0.4649 - acc: 0.5838 - auc_1: 0.6610 - val_loss: 0.4497 - val_acc: 0.6225 - val_auc_1: 0.7063\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.44972, saving model to /app/_data/models/EffB6_cropped/EffB6_1_0.h5\n",
      "Epoch 2/50\n",
      "603/603 [==============================] - 507s 839ms/step - loss: 0.4040 - acc: 0.6500 - auc_1: 0.7173 - val_loss: 0.3825 - val_acc: 0.6583 - val_auc_1: 0.7658\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.44972 to 0.38247, saving model to /app/_data/models/EffB6_cropped/EffB6_1_0.h5\n",
      "Epoch 3/50\n",
      "603/603 [==============================] - 509s 844ms/step - loss: 0.3815 - acc: 0.6645 - auc_1: 0.7607 - val_loss: 0.3766 - val_acc: 0.6475 - val_auc_1: 0.7823\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.38247 to 0.37663, saving model to /app/_data/models/EffB6_cropped/EffB6_1_0.h5\n",
      "Epoch 4/50\n",
      "603/603 [==============================] - 508s 842ms/step - loss: 0.3624 - acc: 0.6786 - auc_1: 0.7888 - val_loss: 0.4302 - val_acc: 0.6050 - val_auc_1: 0.7506\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.37663\n",
      "Epoch 5/50\n",
      "603/603 [==============================] - 513s 849ms/step - loss: 0.3393 - acc: 0.6951 - auc_1: 0.8197 - val_loss: 0.4216 - val_acc: 0.6517 - val_auc_1: 0.7597\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.37663\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0007000000332482159.\n",
      "Epoch 6/50\n",
      "603/603 [==============================] - 513s 850ms/step - loss: 0.2924 - acc: 0.7366 - auc_1: 0.8631 - val_loss: 0.4771 - val_acc: 0.6150 - val_auc_1: 0.7545\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.37663\n",
      "Epoch 7/50\n",
      "603/603 [==============================] - 512s 847ms/step - loss: 0.2349 - acc: 0.7916 - auc_1: 0.9106 - val_loss: 0.5545 - val_acc: 0.5667 - val_auc_1: 0.7322\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.37663\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0004900000232737511.\n",
      "Epoch 8/50\n",
      "603/603 [==============================] - 518s 857ms/step - loss: 0.1909 - acc: 0.8437 - auc_1: 0.9402 - val_loss: 0.4954 - val_acc: 0.5925 - val_auc_1: 0.7609\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.37663\n",
      "Epoch 9/50\n",
      "603/603 [==============================] - 516s 854ms/step - loss: 0.1384 - acc: 0.8912 - auc_1: 0.9617 - val_loss: 0.6049 - val_acc: 0.5750 - val_auc_1: 0.7575\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.37663\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00034300000406801696.\n",
      "Epoch 10/50\n",
      "603/603 [==============================] - 516s 854ms/step - loss: 0.0976 - acc: 0.9247 - auc_1: 0.9785 - val_loss: 0.6747 - val_acc: 0.5950 - val_auc_1: 0.7414\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.37663\n",
      "Epoch 11/50\n",
      "603/603 [==============================] - 517s 857ms/step - loss: 0.0643 - acc: 0.9585 - auc_1: 0.9886 - val_loss: 0.7809 - val_acc: 0.5592 - val_auc_1: 0.7286\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.37663\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00024009999469853935.\n",
      "Epoch 12/50\n",
      "603/603 [==============================] - 516s 855ms/step - loss: 0.0383 - acc: 0.9773 - auc_1: 0.9948 - val_loss: 0.8028 - val_acc: 0.5675 - val_auc_1: 0.7263\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.37663\n",
      "Epoch 13/50\n",
      "603/603 [==============================] - 518s 858ms/step - loss: 0.0221 - acc: 0.9867 - auc_1: 0.9957 - val_loss: 0.9947 - val_acc: 0.5850 - val_auc_1: 0.7129\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.37663\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00016806999628897755.\n",
      "Epoch 14/50\n",
      "603/603 [==============================] - 517s 856ms/step - loss: 0.0123 - acc: 0.9943 - auc_1: 0.9990 - val_loss: 1.0782 - val_acc: 0.6042 - val_auc_1: 0.7131\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.37663\n",
      "Epoch 15/50\n",
      "603/603 [==============================] - 520s 861ms/step - loss: 0.0079 - acc: 0.9959 - auc_1: 0.9987 - val_loss: 1.1327 - val_acc: 0.5883 - val_auc_1: 0.7016\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.37663\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00011764899536501615.\n",
      "Epoch 00015: early stopping\n",
      "\n",
      " epoch #1\n",
      "\n",
      "Warning! Model path already exists.\n",
      "Epoch 1/50\n",
      "603/603 [==============================] - 559s 891ms/step - loss: 0.4683 - acc: 0.5621 - auc: 0.6607 - val_loss: 0.4302 - val_acc: 0.6242 - val_auc: 0.7238\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.43018, saving model to /app/_data/models/EffB6_cropped/EffB6_1_1.h5\n",
      "Epoch 2/50\n",
      "603/603 [==============================] - 518s 858ms/step - loss: 0.4154 - acc: 0.6186 - auc: 0.7255 - val_loss: 0.4140 - val_acc: 0.6075 - val_auc: 0.7362\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.43018 to 0.41402, saving model to /app/_data/models/EffB6_cropped/EffB6_1_1.h5\n",
      "Epoch 3/50\n",
      "603/603 [==============================] - 515s 853ms/step - loss: 0.3841 - acc: 0.6475 - auc: 0.7631 - val_loss: 0.4154 - val_acc: 0.6492 - val_auc: 0.7448\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.41402\n",
      "Epoch 4/50\n",
      "603/603 [==============================] - 520s 860ms/step - loss: 0.3696 - acc: 0.6659 - auc: 0.7849 - val_loss: 0.3974 - val_acc: 0.6250 - val_auc: 0.7575\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.41402 to 0.39743, saving model to /app/_data/models/EffB6_cropped/EffB6_1_1.h5\n",
      "Epoch 5/50\n",
      "603/603 [==============================] - 514s 851ms/step - loss: 0.3399 - acc: 0.6997 - auc: 0.8167 - val_loss: 0.4482 - val_acc: 0.6133 - val_auc: 0.7304\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.39743\n",
      "Epoch 6/50\n",
      "603/603 [==============================] - 517s 856ms/step - loss: 0.3234 - acc: 0.7141 - auc: 0.8451 - val_loss: 0.4083 - val_acc: 0.6283 - val_auc: 0.7539\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.39743\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0007000000332482159.\n",
      "Epoch 7/50\n",
      "603/603 [==============================] - 513s 849ms/step - loss: 0.2565 - acc: 0.7714 - auc: 0.8968 - val_loss: 0.5536 - val_acc: 0.6275 - val_auc: 0.7176\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.39743\n",
      "Epoch 8/50\n",
      "603/603 [==============================] - 511s 847ms/step - loss: 0.1932 - acc: 0.8275 - auc: 0.9364 - val_loss: 0.5467 - val_acc: 0.6108 - val_auc: 0.7394\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.39743\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0004900000232737511.\n",
      "Epoch 9/50\n",
      "603/603 [==============================] - 509s 843ms/step - loss: 0.1532 - acc: 0.8681 - auc: 0.9601 - val_loss: 0.5972 - val_acc: 0.5883 - val_auc: 0.7388\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.39743\n",
      "Epoch 10/50\n",
      "603/603 [==============================] - 515s 853ms/step - loss: 0.1073 - acc: 0.9120 - auc: 0.9747 - val_loss: 0.7741 - val_acc: 0.5858 - val_auc: 0.7264\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.39743\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00034300000406801696.\n",
      "Epoch 11/50\n",
      "603/603 [==============================] - 516s 854ms/step - loss: 0.0824 - acc: 0.9368 - auc: 0.9850 - val_loss: 0.8663 - val_acc: 0.5692 - val_auc: 0.7199\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.39743\n",
      "Epoch 12/50\n",
      "603/603 [==============================] - 557s 923ms/step - loss: 0.0582 - acc: 0.9630 - auc: 0.9911 - val_loss: 0.9306 - val_acc: 0.5675 - val_auc: 0.6996\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.39743\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00024009999469853935.\n",
      "Epoch 13/50\n",
      "603/603 [==============================] - 857s 1s/step - loss: 0.0339 - acc: 0.9772 - auc: 0.9943 - val_loss: 0.9050 - val_acc: 0.5733 - val_auc: 0.6872\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.39743\n",
      "Epoch 14/50\n",
      "603/603 [==============================] - 899s 1s/step - loss: 0.0199 - acc: 0.9904 - auc: 0.9974 - val_loss: 1.1010 - val_acc: 0.5792 - val_auc: 0.6839\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.39743\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00016806999628897755.\n",
      "Epoch 15/50\n",
      "603/603 [==============================] - 951s 2s/step - loss: 0.0169 - acc: 0.9910 - auc: 0.9966 - val_loss: 1.1089 - val_acc: 0.5758 - val_auc: 0.6847\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.39743\n",
      "Epoch 16/50\n",
      "603/603 [==============================] - 965s 2s/step - loss: 0.0077 - acc: 0.9967 - auc: 0.9988 - val_loss: 1.2026 - val_acc: 0.5875 - val_auc: 0.6871\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.39743\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00011764899536501615.\n",
      "Epoch 00016: early stopping\n",
      "\n",
      " epoch #2\n",
      "\n",
      "Warning! Model path already exists.\n",
      "Epoch 1/50\n",
      "603/603 [==============================] - 1018s 2s/step - loss: 0.4660 - acc: 0.5770 - auc: 0.6644 - val_loss: 0.4060 - val_acc: 0.6433 - val_auc: 0.7404\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.40599, saving model to /app/_data/models/EffB6_cropped/EffB6_1_2.h5\n",
      "Epoch 2/50\n",
      "603/603 [==============================] - 1037s 2s/step - loss: 0.4100 - acc: 0.6352 - auc: 0.7153 - val_loss: 0.6412 - val_acc: 0.5058 - val_auc: 0.6845\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.40599\n",
      "Epoch 3/50\n",
      "603/603 [==============================] - 1030s 2s/step - loss: 0.3972 - acc: 0.6465 - auc: 0.7377 - val_loss: 0.4113 - val_acc: 0.6267 - val_auc: 0.7239\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.40599\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0007000000332482159.\n",
      "Epoch 4/50\n",
      "603/603 [==============================] - 1039s 2s/step - loss: 0.3734 - acc: 0.6650 - auc: 0.7806 - val_loss: 0.4288 - val_acc: 0.6342 - val_auc: 0.7545\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.40599\n",
      "Epoch 5/50\n",
      "603/603 [==============================] - 1094s 2s/step - loss: 0.3383 - acc: 0.6925 - auc: 0.8228 - val_loss: 0.4028 - val_acc: 0.6533 - val_auc: 0.7645\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.40599 to 0.40283, saving model to /app/_data/models/EffB6_cropped/EffB6_1_2.h5\n",
      "Epoch 6/50\n",
      "603/603 [==============================] - 1054s 2s/step - loss: 0.2870 - acc: 0.7451 - auc: 0.8721 - val_loss: 0.4642 - val_acc: 0.6358 - val_auc: 0.7380\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.40283\n",
      "Epoch 7/50\n",
      "603/603 [==============================] - 1103s 2s/step - loss: 0.2680 - acc: 0.7714 - auc: 0.8928 - val_loss: 0.5400 - val_acc: 0.5942 - val_auc: 0.7403\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.40283\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0004900000232737511.\n",
      "Epoch 8/50\n",
      "603/603 [==============================] - 1035s 2s/step - loss: 0.1925 - acc: 0.8366 - auc: 0.9328 - val_loss: 0.5335 - val_acc: 0.5983 - val_auc: 0.7401\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.40283\n",
      "Epoch 9/50\n",
      "603/603 [==============================] - 1087s 2s/step - loss: 0.1414 - acc: 0.8851 - auc: 0.9638 - val_loss: 0.6474 - val_acc: 0.5725 - val_auc: 0.7249\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.40283\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00034300000406801696.\n",
      "Epoch 10/50\n",
      "603/603 [==============================] - 1042s 2s/step - loss: 0.1059 - acc: 0.9135 - auc: 0.9769 - val_loss: 0.7224 - val_acc: 0.6108 - val_auc: 0.7260\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.40283\n",
      "Epoch 11/50\n",
      " 64/603 [==>...........................] - ETA: 16:03 - loss: 0.0852 - acc: 0.9352 - auc: 0.9844"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-838d87cc7712>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m     )\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     history = model.fit(\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mgen_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for n in range(0, 5):\n",
    "    print(\"\\n epoch #\" + str(n) + \"\\n\")\n",
    "    val = df.loc[val_ids[n]].sample(frac=1, random_state=base_config[MOD][\"SEED\"])\n",
    "    train = df.loc[train_ids[n]].sample(\n",
    "        frac=1, random_state=base_config[MOD][\"SEED\"]\n",
    "    )\n",
    "\n",
    "    gen_train = Generator(\n",
    "        df=train,\n",
    "        batch_size=base_config[MOD][\"BATCH_SIZE\"],\n",
    "        seed=base_config[MOD][\"SEED\"],\n",
    "        img_size=base_config[MOD][\"IMG_SIZE\"],\n",
    "        cache_img_path=\"/app/_data/crop_npy_528/\",\n",
    "        shuffle=True,\n",
    "        label_columns=[\n",
    "            \"Negative for Pneumonia\",\n",
    "            \"Typical Appearance\",\n",
    "            \"Indeterminate Appearance\",\n",
    "            \"Atypical Appearance\",\n",
    "        ],\n",
    "        augment_fn=Flip_Aug,\n",
    "        crop=True,\n",
    "    )\n",
    "    gen_valid = Generator(\n",
    "        df=val,\n",
    "        batch_size=base_config[MOD][\"BATCH_SIZE\"],\n",
    "        seed=base_config[MOD][\"SEED\"],\n",
    "        img_size=base_config[MOD][\"IMG_SIZE\"],\n",
    "        cache_img_path=\"/app/_data/crop_npy_528/\",\n",
    "        shuffle=False,\n",
    "        label_columns=[\n",
    "            \"Negative for Pneumonia\",\n",
    "            \"Typical Appearance\",\n",
    "            \"Indeterminate Appearance\",\n",
    "            \"Atypical Appearance\",\n",
    "        ],\n",
    "        augment_fn=None,\n",
    "        crop=True,\n",
    "    )\n",
    "\n",
    "    get_m = GetModel(\n",
    "        model_name=MOD,\n",
    "        lr=0.0005,\n",
    "        activation_func=\"softmax\",\n",
    "        weights='imagenet',\n",
    "        n_classes=4,\n",
    "        top_dropout_rate=None,\n",
    "        loss=['bce', keras.losses.CategoricalCrossentropy()],\n",
    "        metrics = [\"acc\", keras.metrics.AUC(multi_label=True)]\n",
    "    )\n",
    "    model = get_m.get_model()\n",
    "\n",
    "    callbacks = get_m.make_callback(\n",
    "        model_path=\"/app/_data/models/EffB6_cropped/\",\n",
    "        model_name=\"EffB6_1_\" + str(n) + \".h5\",\n",
    "        tensorboard_path=\"/app/.tensorboard/EffB6_cr1_\" + str(n),\n",
    "        patience_ES=12,\n",
    "        patience_RLR=2,\n",
    "        factor_LR=0.7,\n",
    "        metric_for_monitor=\"val_loss\",\n",
    "        metric_mode=\"min\",\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        gen_train,\n",
    "        validation_data=gen_valid,\n",
    "        epochs=50,\n",
    "        steps_per_epoch=len(train) // base_config[MOD][\"BATCH_SIZE\"],\n",
    "        validation_steps=len(val) // base_config[MOD][\"BATCH_SIZE\"],\n",
    "        verbose=1,\n",
    "        workers=20,\n",
    "        max_queue_size=500,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db5214d-ea3d-42b9-9f2a-9a5790390a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "score = 0\n",
    "for x in range(4): score += average_precision_score(tst_lbls[:, x], tst_outs[:, x]) / 4 * 2/3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d3da9b-86ff-442a-bd7d-f832c5278178",
   "metadata": {},
   "source": [
    "# evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8add415-7670-4b56-b7a9-46e5355b4bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(\n",
    "    df=df,\n",
    "    batch_size=base_config[\"EFFB7\"][\"BATCH_SIZE\"],\n",
    "    seed=base_config[\"EFFB7\"][\"SEED\"],\n",
    "    img_size=base_config[\"EFFB7\"][\"IMG_SIZE\"],\n",
    "    prepared_img_path=\"/app/_data/train_jpg_600/\",\n",
    "    shuffle=False,\n",
    "    augment=False,\n",
    "    hard_augment=False,\n",
    "    n_inputs=2,\n",
    "    n_classes=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ebdf3f-ddf2-4b40-8b9a-461f8a30d4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_path = \"/app/_data/models/EffB7_init/\"\n",
    "for file in os.listdir(mod_path):\n",
    "    if \".h5\" in file:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a9c1f4f-3d1d-4b4f-aab7-51b94eec20d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1507/1507 [==============================] - 292s 188ms/step - loss: 0.2460 - acc: 0.8874 - recall: 0.8781 - precision: 0.8973 - f1_score: 0.8871\n",
      "1507/1507 [==============================] - 438s 287ms/step - loss: 0.2491 - acc: 0.7890 - recall: 0.7031 - precision: 0.8661 - f1_score: 0.7756\n",
      "1507/1507 [==============================] - 514s 338ms/step - loss: 0.1905 - acc: 0.9041 - recall: 0.8962 - precision: 0.9099 - f1_score: 0.9048\n",
      "1507/1507 [==============================] - 499s 328ms/step - loss: 0.2422 - acc: 0.7971 - recall: 0.7404 - precision: 0.8551 - f1_score: 0.7619\n"
     ]
    }
   ],
   "source": [
    "mod_path = \"/app/_data/models/EffB7_init/\"\n",
    "for file in os.listdir(mod_path):\n",
    "    if \".h5\" in file:\n",
    "        model = keras.models.load_model(mod_path + file)\n",
    "        model.evaluate(gen)\n",
    "        keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22fdecc3-a37f-4734-84ac-577f3460e9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EffB7_3.h5\n",
      "1507/1507 [==============================] - 320s 206ms/step - loss: 0.4167 - acc: 0.6744 - recall: 0.6176 - precision: 0.7273 - f1_score: 0.5826\n",
      "EffB7_0.h5\n",
      "1507/1507 [==============================] - 497s 326ms/step - loss: 0.3747 - acc: 0.6775 - recall: 0.5979 - precision: 0.7653 - f1_score: 0.5872\n",
      "EffB7_1.h5\n",
      "1507/1507 [==============================] - 514s 338ms/step - loss: 0.3693 - acc: 0.6792 - recall: 0.5906 - precision: 0.7759 - f1_score: 0.5906\n",
      "EffB7_2.h5\n",
      "1507/1507 [==============================] - 501s 329ms/step - loss: 0.3743 - acc: 0.6724 - recall: 0.5907 - precision: 0.7444 - f1_score: 0.5885\n",
      "EffB7_4.h5\n",
      "1507/1507 [==============================] - 532s 350ms/step - loss: 0.3499 - acc: 0.6966 - recall: 0.6339 - precision: 0.7642 - f1_score: 0.6068\n"
     ]
    }
   ],
   "source": [
    "mod_path = \"/app/_data/models/EffB7_2/\"\n",
    "for file in os.listdir(mod_path):\n",
    "    if \".h5\" in file:\n",
    "        print(file)\n",
    "        model = keras.models.load_model(mod_path + file)\n",
    "        model.evaluate(gen)\n",
    "        keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9aa73ccb-88fb-4a5b-9d34-e2384f91511f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EffB7_c_1.h5\n",
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "1507/1507 [==============================] - 241s 155ms/step - loss: 0.2588 - acc: 0.7928 - recall: 0.7636 - precision: 0.8212 - f1_score: 0.7631\n",
      "EffB7_c_2.h5\n",
      "1507/1507 [==============================] - 248s 161ms/step - loss: 0.3700 - acc: 0.6749 - recall: 0.6257 - precision: 0.7250 - f1_score: 0.5922\n",
      "EffB7_c_3.h5\n",
      "1507/1507 [==============================] - 248s 160ms/step - loss: 0.3233 - acc: 0.7162 - recall: 0.6478 - precision: 0.8035 - f1_score: 0.6639\n",
      "EffB7_c_4.h5\n",
      "1507/1507 [==============================] - 226s 146ms/step - loss: 0.4215 - acc: 0.6291 - recall: 0.5717 - precision: 0.6708 - f1_score: 0.5457\n",
      "EffB7_c_0_067.h5\n",
      "1507/1507 [==============================] - 207s 134ms/step - loss: 0.3849 - acc: 0.6632 - recall_1: 0.6289 - precision_1: 0.7001 - f1_score: 0.5733\n",
      "EffB7_c_0.h5\n",
      "1507/1507 [==============================] - 511s 336ms/step - loss: 0.3529 - acc: 0.6899 - recall: 0.6276 - precision: 0.7577 - f1_score: 0.6093\n"
     ]
    }
   ],
   "source": [
    "mod_path = \"/app/_data/models/EffB7_comp_data_0_softmax/\"\n",
    "for file in os.listdir(mod_path):\n",
    "    if \".h5\" in file:\n",
    "        print(file)\n",
    "        model = keras.models.load_model(mod_path + file)\n",
    "        model.evaluate(gen)\n",
    "        keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ffa6464-378b-40d1-a5ea-adaabe991553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EffB7_3.h5\n",
      "1507/1507 [==============================] - 375s 246ms/step - loss: 0.3712 - acc: 0.6667 - recall: 0.5738 - precision: 0.7562 - f1_score: 0.5888\n",
      "EffB7_0.h5\n",
      "1507/1507 [==============================] - 529s 348ms/step - loss: 0.3892 - acc: 0.6666 - recall: 0.5941 - precision: 0.7284 - f1_score: 0.5757\n",
      "EffB7_1.h5\n",
      "1507/1507 [==============================] - 528s 347ms/step - loss: 0.4477 - acc: 0.6410 - recall: 0.6012 - precision: 0.6746 - f1_score: 0.5579\n",
      "EffB7_2.h5\n",
      " 865/1507 [================>.............] - ETA: 3:41 - loss: 0.3862 - acc: 0.6731 - recall: 0.5841 - precision: 0.7624 - f1_score: 0.5869"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8eaf7ac26627>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1387\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mod_path = \"/app/_data/models/EffB7_3/\"\n",
    "for file in os.listdir(mod_path):\n",
    "    if \".h5\" in file:\n",
    "        print(file)\n",
    "        model = keras.models.load_model(mod_path + file)\n",
    "        model.evaluate(gen)\n",
    "        keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92184057-abf3-48bd-8436-461299934cab",
   "metadata": {},
   "source": [
    "## evaluation 2 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dc82c20-6da6-4149-a7f4-360879ffa6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen2 = Generator(\n",
    "    df=df,\n",
    "    batch_size=base_config[\"EFFB7\"][\"BATCH_SIZE\"],\n",
    "    seed=base_config[\"EFFB7\"][\"SEED\"],\n",
    "    img_size=base_config[\"EFFB7\"][\"IMG_SIZE\"],\n",
    "    prepared_img_path=\"/app/_data/train_jpg_600/\",\n",
    "    shuffle=False,\n",
    "    augment=False,\n",
    "    hard_augment=False,\n",
    "    n_inputs=2,\n",
    "    n_classes=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b0a5306-2c52-4367-b5ea-88b2367f5861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EffB7_2cl_2.h5\n",
      "1507/1507 [==============================] - 223s 145ms/step - loss: 0.3971 - acc: 0.8230 - recall: 0.8230 - precision: 0.8231 - f1_score: 0.8252\n",
      "EffB7_2cl_1.h5\n",
      "1507/1507 [==============================] - 213s 138ms/step - loss: 0.2898 - acc: 0.8769 - recall: 0.8769 - precision: 0.8769 - f1_score: 0.8733\n",
      "EffB7_2cl_0.h5\n",
      "1507/1507 [==============================] - 518s 341ms/step - loss: 0.4272 - acc: 0.8056 - recall: 0.8056 - precision: 0.8056 - f1_score: 0.8027\n",
      "EffB7_2cl_4.h5\n",
      "1507/1507 [==============================] - 215s 140ms/step - loss: 0.3666 - acc: 0.8442 - recall: 0.8442 - precision: 0.8442 - f1_score: 0.8400\n",
      "EffB7_2cl_3.h5\n",
      "1507/1507 [==============================] - 227s 147ms/step - loss: 0.3073 - acc: 0.8668 - recall: 0.8668 - precision: 0.8668 - f1_score: 0.8628\n"
     ]
    }
   ],
   "source": [
    "mod_path = \"/app/_data/models/EffB7_2class_1/\"\n",
    "for file in os.listdir(mod_path):\n",
    "    if \".h5\" in file:\n",
    "        print(file)\n",
    "        model = keras.models.load_model(mod_path + file)\n",
    "        model.evaluate(gen2)\n",
    "        keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164dc7ca-38cf-4a49-8d14-12f95febd28f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
