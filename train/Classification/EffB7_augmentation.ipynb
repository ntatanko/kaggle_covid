{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4053d71c-d21f-4575-81fc-76dba41a98cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# import efficientnet.tfkeras as efn\n",
    "import albumentations\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom as dicom\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import torch\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from numba import cuda\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.augment import Aug, Aug_Crop, Aug_No_transform, Flip_Aug\n",
    "from src.generator import Generator, GetModel\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0641c0d8-36f2-41c9-9175-6673c380634e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EFFB7': {'IMG_SIZE': 600,\n",
       "  'BATCH_SIZE': 4,\n",
       "  'SEED': 1480,\n",
       "  'WEIGHTS': '/app/_data/noisy-student-efficientnet-b7/efficientnetb7_notop.h5'},\n",
       " 'CLASS': {'negative': 0, 'typical': 1, 'indeterminate': 2, 'atypical': 3},\n",
       " 'EFFB4': {'IMG_SIZE': 380,\n",
       "  'BATCH_SIZE': 50,\n",
       "  'SEED': 42,\n",
       "  'WEIGHTS': '/app/_data/efficientnet-b4_noisy-student_notop.h5'},\n",
       " 'EFFB0': {'IMG_SIZE': 224,\n",
       "  'BATCH_SIZE': 120,\n",
       "  'SEED': 42,\n",
       "  'WEIGHTS': 'imagenet'},\n",
       " 'EFFB6': {'IMG_SIZE': 528,\n",
       "  'BATCH_SIZE': 8,\n",
       "  'SEED': 42,\n",
       "  'WEIGHTS': '/app/_data/noisy-student-efficientnet-b6/efficientnetb6_notop.h5'}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/app/_data/predicted_crop_v1.csv\")\n",
    "list_wrong = df[(df[\"class\"] != \"negative\") & (df[\"label\"] == \"none 1 0 0 1 1\")][\n",
    "    \"id_image\"\n",
    "].tolist()\n",
    "df = df.query(\"id_image not in @list_wrong\").reset_index(drop=True)\n",
    "\n",
    "with open(\"/app/_data/base_config.json\", \"r\") as f:\n",
    "    base_config = json.load(f)\n",
    "base_config[\"EFFB7\"][\"SEED\"] = 1480\n",
    "base_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0800bea2-ad85-48d6-a1b1-55b3485a492b",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f454625-4014-427b-b8a2-8a15005505f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale.py:56: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale\n"
     ]
    }
   ],
   "source": [
    "policy = keras.mixed_precision.experimental.Policy(\"mixed_float16\")\n",
    "keras.mixed_precision.experimental.set_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a7bcd49-fea8-45a1-8bc1-8db9e5904655",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(\n",
    "    n_splits=6, random_state=base_config[\"EFFB7\"][\"SEED\"], shuffle=True\n",
    ")\n",
    "train_ids = []\n",
    "val_ids = []\n",
    "for train_index, valid_index in skf.split(df, df[\"class\"]):\n",
    "    train_ids.append(train_index)\n",
    "    val_ids.append(valid_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a95f3cd3-9caf-47e6-9ee0-46dfee41068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.rmtree('/app/_data/models/EffB7_cropped/backup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7376ba9-0e97-4301-add7-4de7455d31e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aug_Crop:\n",
    "    def augment_image(img):\n",
    "        h = img.shape[0]\n",
    "        w = img.shape[1]\n",
    "        transform = albumentations.Compose(\n",
    "            [\n",
    "#                 albumentations.OneOf(\n",
    "#                     [\n",
    "#                         albumentations.MotionBlur((3, 5)),\n",
    "#                         albumentations.MedianBlur(blur_limit=5),\n",
    "#                         albumentations.GaussianBlur(blur_limit=(3, 5), sigma_limit=0),\n",
    "#                         albumentations.Blur(blur_limit=(3, 5)),\n",
    "#                     ],\n",
    "#                     p=0.1,\n",
    "#                 ),\n",
    "#                 albumentations.OneOf(\n",
    "#                     [\n",
    "#                         albumentations.GaussNoise(var_limit=[10, 20], mean=1),\n",
    "#                         albumentations.ImageCompression(\n",
    "#                             quality_lower=85, quality_upper=100, compression_type=1\n",
    "#                         ),\n",
    "#                         albumentations.MultiplicativeNoise(\n",
    "#                             multiplier=(0.95, 1.05), per_channel=False, elementwise=True\n",
    "#                         ),\n",
    "#                         albumentations.Downscale(\n",
    "#                             scale_min=0.85,\n",
    "#                             scale_max=0.99,\n",
    "#                             interpolation=cv2.INTER_LINEAR,\n",
    "#                         ),\n",
    "#                     ],\n",
    "#                     p=0.1,\n",
    "#                 ),\n",
    "                albumentations.OneOf(\n",
    "                    [\n",
    "                        albumentations.RandomBrightnessContrast(\n",
    "                            brightness_limit=(-0.1, 0.2),\n",
    "                            contrast_limit=(-0.1, 0.2),\n",
    "                            brightness_by_max=True,\n",
    "                        ),\n",
    "#                         albumentations.augmentations.transforms.Sharpen(\n",
    "#                             alpha=(0.05, 0.1), lightness=(0.5, 1.0)\n",
    "#                         ),\n",
    "                        albumentations.augmentations.transforms.RandomToneCurve(\n",
    "                            scale=0.05\n",
    "                        ),\n",
    "                    ],\n",
    "                    p=0.1,\n",
    "                ),\n",
    "#                 albumentations.OneOf(\n",
    "#                     [\n",
    "#                         albumentations.OpticalDistortion(\n",
    "#                             distort_limit=0.1,\n",
    "#                             shift_limit=0.1,\n",
    "#                             border_mode=1,\n",
    "#                         ),\n",
    "#                         albumentations.ElasticTransform(\n",
    "#                             alpha=2.0,\n",
    "#                             sigma=2.0,\n",
    "#                             alpha_affine=2.0,\n",
    "#                             interpolation=cv2.INTER_LINEAR,\n",
    "#                             border_mode=1,\n",
    "#                         ),\n",
    "#                         albumentations.GridDistortion(\n",
    "#                             num_steps=5,\n",
    "#                             distort_limit=0.1,\n",
    "#                             interpolation=cv2.INTER_LINEAR,\n",
    "#                             border_mode=1,\n",
    "#                         ),\n",
    "#                     ],\n",
    "#                     p=0.1,\n",
    "#                 ),\n",
    "                albumentations.OneOf(\n",
    "                    [\n",
    "                        albumentations.augmentations.crops.transforms.CropAndPad(\n",
    "                            px=None,\n",
    "                            percent=(-0.01, 0.05),\n",
    "                            pad_mode=1,\n",
    "                            pad_cval=0,\n",
    "                            pad_cval_mask=0,\n",
    "                            keep_size=True,\n",
    "                            sample_independently=True,\n",
    "                            interpolation=cv2.INTER_LINEAR,\n",
    "                        ),\n",
    "                        albumentations.RandomSizedCrop(\n",
    "                            min_max_height=(0.95 * h, 0.95 * w),\n",
    "                            height=h,\n",
    "                            width=w,\n",
    "                            w2h_ratio=1.0,\n",
    "                            interpolation=cv2.INTER_LINEAR,\n",
    "                        ),\n",
    "                        albumentations.HorizontalFlip(),\n",
    "#                         albumentations.RandomRotate90(),\n",
    "                    ],\n",
    "                    p=0.1,\n",
    "                ),\n",
    "                albumentations.CoarseDropout(\n",
    "                    max_holes=10,\n",
    "                    max_height=60,\n",
    "                    max_width=60,\n",
    "                    min_holes=1,\n",
    "                    min_height=6,\n",
    "                    min_width=6,\n",
    "                    fill_value=0,\n",
    "                    p=0.2\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return transform(image=img)[\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f55d648d-8ba0-4089-9180-24473e538e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_config[\"EFFB7\"][\"BATCH_SIZE\"] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "727f819d-5c9d-4a2b-9611-eecea2ef0f08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " iter #0\n",
      "\n",
      "WARNING:tensorflow:tf.keras.mixed_precision.experimental.LossScaleOptimizer is deprecated. Please use tf.keras.mixed_precision.LossScaleOptimizer instead. Note that the non-experimental LossScaleOptimizer does not take a DynamicLossScale but instead takes the dynamic configuration directly in the constructor. For example:\n",
      "  opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt)\n",
      "\n",
      "Warning! Model path already exists.\n",
      "Epoch 1/50\n",
      "628/628 [==============================] - 323s 388ms/step - loss: 1.1981 - acc: 0.5161 - auc_1: 0.5860 - val_loss: 1.4019 - val_acc: 0.5209 - val_auc_1: 0.6560\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.40189, saving model to /app/_data/models/EffB7_cropped_600_3/EffB7_0.h5\n",
      "Epoch 2/50\n",
      "628/628 [==============================] - 177s 282ms/step - loss: 1.0463 - acc: 0.5923 - auc_1: 0.6662 - val_loss: 0.9845 - val_acc: 0.6384 - val_auc_1: 0.7457\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.40189 to 0.98448, saving model to /app/_data/models/EffB7_cropped_600_3/EffB7_0.h5\n",
      "Epoch 3/50\n",
      "628/628 [==============================] - 175s 278ms/step - loss: 1.0032 - acc: 0.6191 - auc_1: 0.6763 - val_loss: 0.9249 - val_acc: 0.6544 - val_auc_1: 0.7530\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.98448 to 0.92490, saving model to /app/_data/models/EffB7_cropped_600_3/EffB7_0.h5\n",
      "Epoch 4/50\n",
      "628/628 [==============================] - 174s 276ms/step - loss: 1.0193 - acc: 0.6100 - auc_1: 0.6969 - val_loss: 1.0338 - val_acc: 0.6215 - val_auc_1: 0.7309\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.92490\n",
      "Epoch 5/50\n",
      "628/628 [==============================] - 173s 275ms/step - loss: 0.9188 - acc: 0.6598 - auc_1: 0.7255 - val_loss: 0.9471 - val_acc: 0.6434 - val_auc_1: 0.7459\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.92490\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 6/50\n",
      "628/628 [==============================] - 171s 273ms/step - loss: 0.8950 - acc: 0.6631 - auc_1: 0.7798 - val_loss: 0.9267 - val_acc: 0.6434 - val_auc_1: 0.7682\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.92490\n",
      "Epoch 7/50\n",
      "628/628 [==============================] - 173s 275ms/step - loss: 0.8028 - acc: 0.6906 - auc_1: 0.8044 - val_loss: 0.9298 - val_acc: 0.6444 - val_auc_1: 0.7597\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.92490\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 8/50\n",
      "628/628 [==============================] - 172s 273ms/step - loss: 0.7369 - acc: 0.7134 - auc_1: 0.8386 - val_loss: 0.9090 - val_acc: 0.6554 - val_auc_1: 0.7858\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.92490 to 0.90899, saving model to /app/_data/models/EffB7_cropped_600_3/EffB7_0.h5\n",
      "Epoch 9/50\n",
      "628/628 [==============================] - 173s 275ms/step - loss: 0.6285 - acc: 0.7508 - auc_1: 0.8784 - val_loss: 1.0428 - val_acc: 0.6474 - val_auc_1: 0.7730\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.90899\n",
      "Epoch 10/50\n",
      "628/628 [==============================] - 173s 275ms/step - loss: 0.6663 - acc: 0.7372 - auc_1: 0.8766 - val_loss: 1.1060 - val_acc: 0.5976 - val_auc_1: 0.7482\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.90899\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 11/50\n",
      "628/628 [==============================] - 173s 275ms/step - loss: 0.5669 - acc: 0.7876 - auc_1: 0.9010 - val_loss: 1.1438 - val_acc: 0.6036 - val_auc_1: 0.7630\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.90899\n",
      "Epoch 12/50\n",
      "628/628 [==============================] - 173s 275ms/step - loss: 0.4544 - acc: 0.8332 - auc_1: 0.9289 - val_loss: 1.2671 - val_acc: 0.5956 - val_auc_1: 0.7543\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.90899\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 13/50\n",
      "628/628 [==============================] - 174s 276ms/step - loss: 0.3914 - acc: 0.8529 - auc_1: 0.9417 - val_loss: 1.3294 - val_acc: 0.5936 - val_auc_1: 0.7602\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.90899\n",
      "Epoch 00013: early stopping\n",
      "\n",
      " iter #1\n",
      "\n",
      "WARNING:tensorflow:tf.keras.mixed_precision.experimental.LossScaleOptimizer is deprecated. Please use tf.keras.mixed_precision.LossScaleOptimizer instead. Note that the non-experimental LossScaleOptimizer does not take a DynamicLossScale but instead takes the dynamic configuration directly in the constructor. For example:\n",
      "  opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt)\n",
      "\n",
      "Warning! Model path already exists.\n",
      "Epoch 1/50\n",
      "628/628 [==============================] - 222s 289ms/step - loss: 1.2130 - acc: 0.5059 - auc: 0.5860 - val_loss: 1.0255 - val_acc: 0.6056 - val_auc: 0.7050\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02547, saving model to /app/_data/models/EffB7_cropped_600_3/EffB7_1.h5\n",
      "Epoch 2/50\n",
      "628/628 [==============================] - 172s 273ms/step - loss: 1.0829 - acc: 0.5802 - auc: 0.6378 - val_loss: 1.3697 - val_acc: 0.3406 - val_auc: 0.6160\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.02547\n",
      "Epoch 3/50\n",
      "628/628 [==============================] - 172s 273ms/step - loss: 1.0246 - acc: 0.6164 - auc: 0.6823 - val_loss: 1.1771 - val_acc: 0.5199 - val_auc: 0.6872\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.02547\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 4/50\n",
      "628/628 [==============================] - 173s 273ms/step - loss: 0.9571 - acc: 0.6228 - auc: 0.7184 - val_loss: 1.2411 - val_acc: 0.5279 - val_auc: 0.6994\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.02547\n",
      "Epoch 5/50\n",
      "628/628 [==============================] - 173s 274ms/step - loss: 0.9207 - acc: 0.6520 - auc: 0.7562 - val_loss: 0.9153 - val_acc: 0.6574 - val_auc: 0.7746\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.02547 to 0.91528, saving model to /app/_data/models/EffB7_cropped_600_3/EffB7_1.h5\n",
      "Epoch 6/50\n",
      "628/628 [==============================] - 171s 273ms/step - loss: 0.8421 - acc: 0.6842 - auc: 0.7867 - val_loss: 0.9499 - val_acc: 0.6046 - val_auc: 0.7767\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.91528\n",
      "Epoch 7/50\n",
      "628/628 [==============================] - 173s 274ms/step - loss: 0.8456 - acc: 0.6802 - auc: 0.7753 - val_loss: 0.9029 - val_acc: 0.6444 - val_auc: 0.7676\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.91528 to 0.90294, saving model to /app/_data/models/EffB7_cropped_600_3/EffB7_1.h5\n",
      "Epoch 8/50\n",
      "628/628 [==============================] - 172s 274ms/step - loss: 0.7566 - acc: 0.6975 - auc: 0.8288 - val_loss: 0.9854 - val_acc: 0.6494 - val_auc: 0.7635\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.90294\n",
      "Epoch 9/50\n",
      "628/628 [==============================] - 173s 274ms/step - loss: 0.7225 - acc: 0.7171 - auc: 0.8512 - val_loss: 0.9477 - val_acc: 0.6464 - val_auc: 0.7616\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.90294\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 10/50\n",
      "628/628 [==============================] - 173s 275ms/step - loss: 0.6304 - acc: 0.7583 - auc: 0.8882 - val_loss: 1.0198 - val_acc: 0.6434 - val_auc: 0.7668\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.90294\n",
      "Epoch 11/50\n",
      "628/628 [==============================] - 172s 273ms/step - loss: 0.5095 - acc: 0.8154 - auc: 0.9127 - val_loss: 1.0825 - val_acc: 0.6434 - val_auc: 0.7637\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.90294\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 12/50\n",
      "628/628 [==============================] - 173s 275ms/step - loss: 0.4820 - acc: 0.8186 - auc: 0.9300 - val_loss: 1.2015 - val_acc: 0.6235 - val_auc: 0.7686\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.90294\n",
      "Epoch 00012: early stopping\n",
      "\n",
      " iter #2\n",
      "\n",
      "WARNING:tensorflow:tf.keras.mixed_precision.experimental.LossScaleOptimizer is deprecated. Please use tf.keras.mixed_precision.LossScaleOptimizer instead. Note that the non-experimental LossScaleOptimizer does not take a DynamicLossScale but instead takes the dynamic configuration directly in the constructor. For example:\n",
      "  opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt)\n",
      "\n",
      "Warning! Model path already exists.\n",
      "Epoch 1/50\n",
      "628/628 [==============================] - 224s 292ms/step - loss: 1.2290 - acc: 0.4818 - auc: 0.5900 - val_loss: 1.1651 - val_acc: 0.6016 - val_auc: 0.6791\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.16513, saving model to /app/_data/models/EffB7_cropped_600_3/EffB7_2.h5\n",
      "Epoch 2/50\n",
      "628/628 [==============================] - 170s 270ms/step - loss: 1.1044 - acc: 0.5675 - auc: 0.6446 - val_loss: 0.9994 - val_acc: 0.6026 - val_auc: 0.7176\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.16513 to 0.99937, saving model to /app/_data/models/EffB7_cropped_600_3/EffB7_2.h5\n",
      "Epoch 3/50\n",
      "628/628 [==============================] - 171s 272ms/step - loss: 1.0713 - acc: 0.5835 - auc: 0.6652 - val_loss: 1.0388 - val_acc: 0.6046 - val_auc: 0.6790\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.99937\n",
      "Epoch 4/50\n",
      "628/628 [==============================] - 173s 273ms/step - loss: 1.0259 - acc: 0.6188 - auc: 0.6807 - val_loss: 1.6861 - val_acc: 0.4124 - val_auc: 0.5722\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.99937\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 5/50\n",
      "628/628 [==============================] - 172s 273ms/step - loss: 0.9467 - acc: 0.6417 - auc: 0.7285 - val_loss: 0.9401 - val_acc: 0.6474 - val_auc: 0.7631\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.99937 to 0.94009, saving model to /app/_data/models/EffB7_cropped_600_3/EffB7_2.h5\n",
      "Epoch 6/50\n",
      "628/628 [==============================] - 172s 273ms/step - loss: 0.9213 - acc: 0.6563 - auc: 0.7488 - val_loss: 0.9636 - val_acc: 0.6135 - val_auc: 0.7554\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.94009\n",
      "Epoch 7/50\n",
      "628/628 [==============================] - 172s 273ms/step - loss: 0.9040 - acc: 0.6450 - auc: 0.7667 - val_loss: 0.9920 - val_acc: 0.6135 - val_auc: 0.7448\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.94009\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 8/50\n",
      "628/628 [==============================] - 172s 273ms/step - loss: 0.8370 - acc: 0.6798 - auc: 0.8058 - val_loss: 0.9397 - val_acc: 0.6424 - val_auc: 0.7693\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.94009 to 0.93970, saving model to /app/_data/models/EffB7_cropped_600_3/EffB7_2.h5\n",
      "Epoch 9/50\n",
      "628/628 [==============================] - 170s 271ms/step - loss: 0.7448 - acc: 0.7062 - auc: 0.8375 - val_loss: 1.0920 - val_acc: 0.6026 - val_auc: 0.7443\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.93970\n",
      "Epoch 10/50\n",
      "628/628 [==============================] - 171s 272ms/step - loss: 0.6862 - acc: 0.7343 - auc: 0.8729 - val_loss: 1.0452 - val_acc: 0.6116 - val_auc: 0.7635\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.93970\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 11/50\n",
      "628/628 [==============================] - 173s 275ms/step - loss: 0.6087 - acc: 0.7566 - auc: 0.8915 - val_loss: 1.0369 - val_acc: 0.6096 - val_auc: 0.7715\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.93970\n",
      "Epoch 12/50\n",
      "628/628 [==============================] - 173s 274ms/step - loss: 0.5535 - acc: 0.7887 - auc: 0.9121 - val_loss: 1.0880 - val_acc: 0.6145 - val_auc: 0.7693\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.93970\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 13/50\n",
      "628/628 [==============================] - 172s 274ms/step - loss: 0.4630 - acc: 0.8232 - auc: 0.9333 - val_loss: 1.1758 - val_acc: 0.6026 - val_auc: 0.7665\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.93970\n",
      "Epoch 00013: early stopping\n",
      "\n",
      " iter #3\n",
      "\n",
      "WARNING:tensorflow:tf.keras.mixed_precision.experimental.LossScaleOptimizer is deprecated. Please use tf.keras.mixed_precision.LossScaleOptimizer instead. Note that the non-experimental LossScaleOptimizer does not take a DynamicLossScale but instead takes the dynamic configuration directly in the constructor. For example:\n",
      "  opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt)\n",
      "\n",
      "Warning! Model path already exists.\n",
      "Epoch 1/50\n",
      "628/628 [==============================] - 223s 292ms/step - loss: 1.2023 - acc: 0.5124 - auc: 0.5951 - val_loss: 1.1454 - val_acc: 0.5886 - val_auc: 0.6848\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.14538, saving model to /app/_data/models/EffB7_cropped_600_3/EffB7_3.h5\n",
      "Epoch 2/50\n",
      "628/628 [==============================] - 173s 274ms/step - loss: 1.0460 - acc: 0.5993 - auc: 0.6446 - val_loss: 1.2244 - val_acc: 0.4641 - val_auc: 0.6687\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.14538\n",
      "Epoch 3/50\n",
      "628/628 [==============================] - 171s 273ms/step - loss: 1.0282 - acc: 0.6096 - auc: 0.6854 - val_loss: 1.4014 - val_acc: 0.4761 - val_auc: 0.6738\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.14538\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 4/50\n",
      "628/628 [==============================] - 172s 274ms/step - loss: 0.9566 - acc: 0.6293 - auc: 0.7251 - val_loss: 0.9119 - val_acc: 0.6564 - val_auc: 0.7585\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.14538 to 0.91188, saving model to /app/_data/models/EffB7_cropped_600_3/EffB7_3.h5\n",
      "Epoch 5/50\n",
      "628/628 [==============================] - 173s 275ms/step - loss: 0.9266 - acc: 0.6425 - auc: 0.7475 - val_loss: 0.9328 - val_acc: 0.6484 - val_auc: 0.7666\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.91188\n",
      "Epoch 6/50\n",
      "628/628 [==============================] - 172s 273ms/step - loss: 0.8884 - acc: 0.6651 - auc: 0.7705 - val_loss: 0.9402 - val_acc: 0.6544 - val_auc: 0.7454\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.91188\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 7/50\n",
      "628/628 [==============================] - 172s 274ms/step - loss: 0.7888 - acc: 0.7063 - auc: 0.8103 - val_loss: 0.9065 - val_acc: 0.6604 - val_auc: 0.7750\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.91188 to 0.90653, saving model to /app/_data/models/EffB7_cropped_600_3/EffB7_3.h5\n",
      "Epoch 8/50\n",
      "628/628 [==============================] - 172s 274ms/step - loss: 0.7425 - acc: 0.7158 - auc: 0.8289 - val_loss: 0.9887 - val_acc: 0.6285 - val_auc: 0.7496\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.90653\n",
      "Epoch 9/50\n",
      "628/628 [==============================] - 173s 276ms/step - loss: 0.7042 - acc: 0.7374 - auc: 0.8574 - val_loss: 0.9753 - val_acc: 0.6653 - val_auc: 0.7710\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.90653\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 10/50\n",
      "628/628 [==============================] - 173s 275ms/step - loss: 0.6042 - acc: 0.7688 - auc: 0.8891 - val_loss: 1.0327 - val_acc: 0.6584 - val_auc: 0.7667\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.90653\n",
      "Epoch 11/50\n",
      "628/628 [==============================] - 173s 275ms/step - loss: 0.5376 - acc: 0.8102 - auc: 0.8976 - val_loss: 1.0902 - val_acc: 0.6275 - val_auc: 0.7556\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.90653\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 12/50\n",
      "628/628 [==============================] - 171s 272ms/step - loss: 0.4739 - acc: 0.8263 - auc: 0.9250 - val_loss: 1.2000 - val_acc: 0.6165 - val_auc: 0.7542\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.90653\n",
      "Epoch 00012: early stopping\n",
      "\n",
      " iter #4\n",
      "\n",
      "WARNING:tensorflow:tf.keras.mixed_precision.experimental.LossScaleOptimizer is deprecated. Please use tf.keras.mixed_precision.LossScaleOptimizer instead. Note that the non-experimental LossScaleOptimizer does not take a DynamicLossScale but instead takes the dynamic configuration directly in the constructor. For example:\n",
      "  opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt)\n",
      "\n",
      "Warning! Model path already exists.\n",
      "Epoch 1/50\n",
      "628/628 [==============================] - 223s 293ms/step - loss: 1.1715 - acc: 0.5318 - auc: 0.6108 - val_loss: 1.0749 - val_acc: 0.5538 - val_auc: 0.6836\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.07491, saving model to /app/_data/models/EffB7_cropped_600_3/EffB7_4.h5\n",
      "Epoch 2/50\n",
      "628/628 [==============================] - 172s 273ms/step - loss: 1.0764 - acc: 0.5860 - auc: 0.6494 - val_loss: 0.9662 - val_acc: 0.6444 - val_auc: 0.7282\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.07491 to 0.96617, saving model to /app/_data/models/EffB7_cropped_600_3/EffB7_4.h5\n",
      "Epoch 3/50\n",
      "628/628 [==============================] - 173s 275ms/step - loss: 1.0566 - acc: 0.5910 - auc: 0.6810 - val_loss: 0.9846 - val_acc: 0.6225 - val_auc: 0.7366\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.96617\n",
      "Epoch 4/50\n",
      "628/628 [==============================] - 172s 273ms/step - loss: 0.9755 - acc: 0.6330 - auc: 0.6894 - val_loss: 0.9568 - val_acc: 0.6305 - val_auc: 0.7475\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.96617 to 0.95681, saving model to /app/_data/models/EffB7_cropped_600_3/EffB7_4.h5\n",
      "Epoch 5/50\n",
      "628/628 [==============================] - 173s 275ms/step - loss: 0.9616 - acc: 0.6295 - auc: 0.7325 - val_loss: 0.9578 - val_acc: 0.6285 - val_auc: 0.7449\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.95681\n",
      "Epoch 6/50\n",
      "628/628 [==============================] - 173s 276ms/step - loss: 0.9348 - acc: 0.6396 - auc: 0.7380 - val_loss: 1.1652 - val_acc: 0.5129 - val_auc: 0.7112\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.95681\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 7/50\n",
      "628/628 [==============================] - 173s 275ms/step - loss: 0.8883 - acc: 0.6544 - auc: 0.7784 - val_loss: 0.9616 - val_acc: 0.6325 - val_auc: 0.7641\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.95681\n",
      "Epoch 8/50\n",
      "628/628 [==============================] - 173s 275ms/step - loss: 0.8093 - acc: 0.6969 - auc: 0.7991 - val_loss: 0.9126 - val_acc: 0.6355 - val_auc: 0.7774\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.95681 to 0.91263, saving model to /app/_data/models/EffB7_cropped_600_3/EffB7_4.h5\n",
      "Epoch 9/50\n",
      "628/628 [==============================] - 171s 272ms/step - loss: 0.7734 - acc: 0.7041 - auc: 0.8411 - val_loss: 1.1183 - val_acc: 0.6106 - val_auc: 0.7370\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.91263\n",
      "Epoch 10/50\n",
      "628/628 [==============================] - 176s 280ms/step - loss: 0.7335 - acc: 0.7299 - auc: 0.8362 - val_loss: 0.9594 - val_acc: 0.6404 - val_auc: 0.7621\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.91263\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 11/50\n",
      "628/628 [==============================] - 177s 282ms/step - loss: 0.6198 - acc: 0.7743 - auc: 0.8810 - val_loss: 1.0825 - val_acc: 0.6145 - val_auc: 0.7551\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.91263\n",
      "Epoch 12/50\n",
      "628/628 [==============================] - 186s 295ms/step - loss: 0.5835 - acc: 0.7823 - auc: 0.9081 - val_loss: 1.0974 - val_acc: 0.6205 - val_auc: 0.7674\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.91263\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 13/50\n",
      "628/628 [==============================] - 184s 292ms/step - loss: 0.4427 - acc: 0.8327 - auc: 0.9353 - val_loss: 1.2055 - val_acc: 0.5916 - val_auc: 0.7613\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.91263\n",
      "Epoch 00013: early stopping\n",
      "\n",
      " iter #5\n",
      "\n",
      "WARNING:tensorflow:tf.keras.mixed_precision.experimental.LossScaleOptimizer is deprecated. Please use tf.keras.mixed_precision.LossScaleOptimizer instead. Note that the non-experimental LossScaleOptimizer does not take a DynamicLossScale but instead takes the dynamic configuration directly in the constructor. For example:\n",
      "  opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt)\n",
      "\n",
      "Warning! Model path already exists.\n",
      "Epoch 1/50\n",
      "628/628 [==============================] - 227s 298ms/step - loss: 1.2291 - acc: 0.4828 - auc: 0.5516 - val_loss: 1.2414 - val_acc: 0.5717 - val_auc: 0.6174\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.24143, saving model to /app/_data/models/EffB7_cropped_600_3/EffB7_5.h5\n",
      "Epoch 2/50\n",
      "628/628 [==============================] - 173s 275ms/step - loss: 1.0891 - acc: 0.5881 - auc: 0.6530 - val_loss: 1.0276 - val_acc: 0.6195 - val_auc: 0.7123\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.24143 to 1.02763, saving model to /app/_data/models/EffB7_cropped_600_3/EffB7_5.h5\n",
      "Epoch 3/50\n",
      "628/628 [==============================] - 174s 275ms/step - loss: 1.0252 - acc: 0.6213 - auc: 0.6870 - val_loss: 0.9711 - val_acc: 0.6494 - val_auc: 0.7458\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02763 to 0.97109, saving model to /app/_data/models/EffB7_cropped_600_3/EffB7_5.h5\n",
      "Epoch 4/50\n",
      "628/628 [==============================] - 174s 277ms/step - loss: 0.9743 - acc: 0.6315 - auc: 0.7112 - val_loss: 1.0457 - val_acc: 0.6116 - val_auc: 0.6989\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.97109\n",
      "Epoch 5/50\n",
      "628/628 [==============================] - 173s 275ms/step - loss: 0.9451 - acc: 0.6387 - auc: 0.7356 - val_loss: 0.9812 - val_acc: 0.6315 - val_auc: 0.7319\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.97109\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 6/50\n",
      "628/628 [==============================] - 173s 275ms/step - loss: 0.8613 - acc: 0.6763 - auc: 0.7759 - val_loss: 0.9422 - val_acc: 0.6444 - val_auc: 0.7586\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.97109 to 0.94222, saving model to /app/_data/models/EffB7_cropped_600_3/EffB7_5.h5\n",
      "Epoch 7/50\n",
      "628/628 [==============================] - 173s 275ms/step - loss: 0.8418 - acc: 0.6667 - auc: 0.8027 - val_loss: 0.9529 - val_acc: 0.6355 - val_auc: 0.7590\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.94222\n",
      "Epoch 8/50\n",
      "628/628 [==============================] - 173s 275ms/step - loss: 0.7941 - acc: 0.6922 - auc: 0.8210 - val_loss: 0.9757 - val_acc: 0.6295 - val_auc: 0.7640\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.94222\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 9/50\n",
      "628/628 [==============================] - 173s 275ms/step - loss: 0.7221 - acc: 0.7110 - auc: 0.8567 - val_loss: 0.9771 - val_acc: 0.6444 - val_auc: 0.7709\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.94222\n",
      "Epoch 10/50\n",
      "628/628 [==============================] - 174s 276ms/step - loss: 0.6362 - acc: 0.7455 - auc: 0.8858 - val_loss: 0.9948 - val_acc: 0.6444 - val_auc: 0.7675\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.94222\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 11/50\n",
      "628/628 [==============================] - 173s 275ms/step - loss: 0.5055 - acc: 0.8043 - auc: 0.9220 - val_loss: 1.1019 - val_acc: 0.6205 - val_auc: 0.7601\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.94222\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "for n in range(0, 6):\n",
    "    print(\"\\n iter #\" + str(n) + \"\\n\")\n",
    "    val = df.loc[val_ids[n]].sample(frac=1, random_state=base_config[\"EFFB7\"][\"SEED\"])\n",
    "    train = df.loc[train_ids[n]].sample(\n",
    "        frac=1, random_state=base_config[\"EFFB7\"][\"SEED\"]\n",
    "    )\n",
    "\n",
    "    gen_train = Generator(\n",
    "        df=train,\n",
    "        batch_size=base_config[\"EFFB7\"][\"BATCH_SIZE\"],\n",
    "        seed=base_config[\"EFFB7\"][\"SEED\"],\n",
    "        img_size=600,\n",
    "        cache_img_path=\"/app/_data/crop_npy_600/\",\n",
    "        shuffle=True,\n",
    "        label_columns=[\n",
    "            \"Negative for Pneumonia\",\n",
    "            \"Typical Appearance\",\n",
    "            \"Indeterminate Appearance\",\n",
    "            \"Atypical Appearance\",\n",
    "        ],\n",
    "        augment_fn=Aug_Crop,\n",
    "        crop=True,\n",
    "    )\n",
    "    gen_valid = Generator(\n",
    "        df=val,\n",
    "        batch_size=base_config[\"EFFB7\"][\"BATCH_SIZE\"],\n",
    "        seed=base_config[\"EFFB7\"][\"SEED\"],\n",
    "        img_size=600,\n",
    "        cache_img_path=\"/app/_data/crop_npy_600/\",\n",
    "        shuffle=False,\n",
    "        label_columns=[\n",
    "            \"Negative for Pneumonia\",\n",
    "            \"Typical Appearance\",\n",
    "            \"Indeterminate Appearance\",\n",
    "            \"Atypical Appearance\",\n",
    "        ],\n",
    "        augment_fn=None,\n",
    "        crop=True,\n",
    "    )\n",
    "\n",
    "    get_m = GetModel(\n",
    "        model_name=\"EFFB7\",\n",
    "        lr=0.0005,\n",
    "        activation_func=\"softmax\",\n",
    "        weights=base_config[\"EFFB7\"][\"WEIGHTS\"],\n",
    "        n_classes=4,\n",
    "        top_dropout_rate=0.5,\n",
    "        loss=['categorical_crossentropy'],\n",
    "        metrics=[\"acc\", keras.metrics.AUC(multi_label=True)],\n",
    "    )\n",
    "    model = get_m.get_model()\n",
    "\n",
    "    callbacks = get_m.make_callback(\n",
    "        model_path=\"/app/_data/models/EffB7_cropped_600_3/\",\n",
    "        model_name=\"EffB7_\" + str(n) + \".h5\",\n",
    "        tensorboard_path=\"/app/.tensorboard/EffB7_cropped_32_\" + str(n),\n",
    "        patience_ES=5,\n",
    "        patience_RLR=2,\n",
    "        factor_LR=0.5,\n",
    "        metric_for_monitor=\"val_loss\",\n",
    "        metric_mode=\"min\",\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        gen_train,\n",
    "        validation_data=gen_valid,\n",
    "        epochs=50,\n",
    "        steps_per_epoch=len(train) // base_config[\"EFFB7\"][\"BATCH_SIZE\"]//2,\n",
    "        validation_steps=len(val) // base_config[\"EFFB7\"][\"BATCH_SIZE\"],\n",
    "        verbose=1,\n",
    "        workers=20,\n",
    "        max_queue_size=500,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    keras.backend.clear_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
