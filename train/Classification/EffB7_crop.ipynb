{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d828387f-f568-4554-b7f5-4bb47207e1a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import albumentations\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom as dicom\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import torch\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from numba import cuda\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.augment import Aug, Aug_Crop, Aug_No_transform, Flip_Aug\n",
    "from src.generator import Generator, GetModel\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6706ef9-b49f-4b8b-908e-a4aa69102c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EFFB7': {'IMG_SIZE': 600,\n",
       "  'BATCH_SIZE': 4,\n",
       "  'SEED': 98,\n",
       "  'WEIGHTS': '/app/_data/noisy-student-efficientnet-b7/efficientnetb7_notop.h5'},\n",
       " 'CLASS': {'negative': 0, 'typical': 1, 'indeterminate': 2, 'atypical': 3},\n",
       " 'EFFB4': {'IMG_SIZE': 380,\n",
       "  'BATCH_SIZE': 50,\n",
       "  'SEED': 42,\n",
       "  'WEIGHTS': '/app/_data/efficientnet-b4_noisy-student_notop.h5'},\n",
       " 'EFFB0': {'IMG_SIZE': 224,\n",
       "  'BATCH_SIZE': 120,\n",
       "  'SEED': 42,\n",
       "  'WEIGHTS': 'imagenet'},\n",
       " 'EFFB6': {'IMG_SIZE': 528,\n",
       "  'BATCH_SIZE': 8,\n",
       "  'SEED': 42,\n",
       "  'WEIGHTS': '/app/_data/noisy-student-efficientnet-b6/efficientnetb6_notop.h5'}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/app/_data/predicted_crop_v1.csv\")\n",
    "list_wrong = df[(df[\"class\"] != \"negative\") & (df[\"label\"] == \"none 1 0 0 1 1\")][\n",
    "    \"id_image\"\n",
    "].tolist()\n",
    "df = df.query(\"id_image not in @list_wrong\").reset_index(drop=True)\n",
    "\n",
    "with open(\"/app/_data/base_config.json\", \"r\") as f:\n",
    "    base_config = json.load(f)\n",
    "base_config[\"EFFB7\"][\"SEED\"] = 98\n",
    "base_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "252d0e91-8ddc-4fe4-b2ba-437db31dc3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output1': [0.9121543778801844,\n",
       "  0.5266045892916528,\n",
       "  1.4291516245487366,\n",
       "  3.278467908902692],\n",
       " 'output2': [1.8243087557603688, 0.6887777294475859]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_for_negative = (1 / sum(df[\"class\"] == \"negative\")) * (df.shape[0] / 4.0)\n",
    "weight_for_typical = (1 / sum(df[\"class\"] == \"typical\")) * (df.shape[0] / 4.0)\n",
    "weight_for_indeterminate = (1 / sum(df[\"class\"] == \"indeterminate\")) * (\n",
    "    df.shape[0] / 4.0\n",
    ")\n",
    "weight_for_atypical = (1 / sum(df[\"class\"] == \"atypical\")) * (df.shape[0] / 4.0)\n",
    "non_negative = (1 / sum(df[\"class\"] != \"negative\")) * (df.shape[0] / 2.0)\n",
    "negative = (1 / sum(df[\"class\"] == \"negative\")) * (df.shape[0] / 2.0)\n",
    "class_weights = {\n",
    "    \"output1\": [\n",
    "        weight_for_negative,\n",
    "        weight_for_typical,\n",
    "        weight_for_indeterminate,\n",
    "        weight_for_atypical,\n",
    "    ],\n",
    "    \"output2\": [negative, non_negative],\n",
    "}\n",
    "class_weights\n",
    "loss_weights = {\"output1\": 1, \"output2\": 0.1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ead6122-7ad5-4da6-a3e1-ff1e8d2892f9",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74ade85a-362d-41d9-9f92-0911de12a51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale.py:56: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale\n"
     ]
    }
   ],
   "source": [
    "policy = keras.mixed_precision.experimental.Policy(\"mixed_float16\")\n",
    "keras.mixed_precision.experimental.set_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1077918-8c4e-4be4-afdb-3306a867f521",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(\n",
    "    n_splits=5, random_state=base_config[\"EFFB7\"][\"SEED\"], shuffle=True\n",
    ")\n",
    "train_ids = []\n",
    "val_ids = []\n",
    "for train_index, valid_index in skf.split(df, df[\"class\"]):\n",
    "    train_ids.append(train_index)\n",
    "    val_ids.append(valid_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a07f85f-0b5c-429d-92dc-e4a6d003dc2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " epoch #0\n",
      "\n",
      "WARNING:tensorflow:tf.keras.mixed_precision.experimental.LossScaleOptimizer is deprecated. Please use tf.keras.mixed_precision.LossScaleOptimizer instead. Note that the non-experimental LossScaleOptimizer does not take a DynamicLossScale but instead takes the dynamic configuration directly in the constructor. For example:\n",
      "  opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt)\n",
      "\n",
      "Warning! Model path already exists.\n",
      "Epoch 5/50\n",
      "1266/1266 [==============================] - 479s 314ms/step - loss: 1.4548 - acc: 0.4025 - val_loss: 2.0591 - val_acc: 0.4581\n",
      "\n",
      "Epoch 00005: val_loss improved from inf to 2.05906, saving model to /app/_data/models/EffB7_cropped/EffB7_0.h5\n",
      "Epoch 6/50\n",
      "1266/1266 [==============================] - 326s 257ms/step - loss: 1.4486 - acc: 0.4037 - val_loss: 1.2887 - val_acc: 0.4755\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.05906 to 1.28873, saving model to /app/_data/models/EffB7_cropped/EffB7_0.h5\n",
      "Epoch 7/50\n",
      "1266/1266 [==============================] - 326s 257ms/step - loss: 1.4733 - acc: 0.4231 - val_loss: nan - val_acc: 0.4842\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.28873\n",
      "Epoch 8/50\n",
      "1266/1266 [==============================] - 327s 258ms/step - loss: 1.4538 - acc: 0.4149 - val_loss: nan - val_acc: 0.4628\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.28873\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00044799996539950375.\n",
      "Epoch 9/50\n",
      "1266/1266 [==============================] - 327s 258ms/step - loss: 1.3610 - acc: 0.4463 - val_loss: nan - val_acc: 0.5427\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.28873\n",
      "Epoch 10/50\n",
      "1266/1266 [==============================] - 325s 257ms/step - loss: 1.3150 - acc: 0.4699 - val_loss: nan - val_acc: 0.4304\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.28873\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00035839998163282876.\n",
      "Epoch 11/50\n",
      "1266/1266 [==============================] - 326s 257ms/step - loss: 1.2200 - acc: 0.5348 - val_loss: nan - val_acc: 0.5554\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.28873\n",
      "Epoch 12/50\n",
      "1266/1266 [==============================] - 329s 260ms/step - loss: 1.2520 - acc: 0.5181 - val_loss: 1.0810 - val_acc: 0.5910\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.28873 to 1.08103, saving model to /app/_data/models/EffB7_cropped/EffB7_0.h5\n",
      "Epoch 13/50\n",
      "1266/1266 [==============================] - 326s 257ms/step - loss: 1.2102 - acc: 0.5264 - val_loss: nan - val_acc: 0.6005\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.08103\n",
      "Epoch 14/50\n",
      "1266/1266 [==============================] - 328s 258ms/step - loss: 1.1764 - acc: 0.5572 - val_loss: nan - val_acc: 0.5625\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.08103\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0002867199946194887.\n",
      "Epoch 15/50\n",
      "1266/1266 [==============================] - 331s 261ms/step - loss: 1.1508 - acc: 0.5597 - val_loss: nan - val_acc: 0.5665\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.08103\n",
      "Epoch 16/50\n",
      "1266/1266 [==============================] - 327s 258ms/step - loss: 1.1094 - acc: 0.5910 - val_loss: nan - val_acc: 0.5997\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.08103\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00022937599569559098.\n",
      "Epoch 17/50\n",
      "1266/1266 [==============================] - 328s 258ms/step - loss: 1.0632 - acc: 0.5968 - val_loss: nan - val_acc: 0.5973\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.08103\n",
      "Epoch 18/50\n",
      "1266/1266 [==============================] - 328s 259ms/step - loss: 1.0667 - acc: 0.5923 - val_loss: nan - val_acc: 0.6036\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.08103\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0001835007918998599.\n",
      "Epoch 19/50\n",
      "1266/1266 [==============================] - 323s 255ms/step - loss: 1.0152 - acc: 0.6183 - val_loss: nan - val_acc: 0.5949\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.08103\n",
      "Epoch 20/50\n",
      "1266/1266 [==============================] - 327s 258ms/step - loss: 0.9886 - acc: 0.6283 - val_loss: nan - val_acc: 0.6005\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.08103\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00014680062886327506.\n",
      "Epoch 21/50\n",
      "1266/1266 [==============================] - 323s 255ms/step - loss: 0.9462 - acc: 0.6390 - val_loss: nan - val_acc: 0.5934\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.08103\n",
      "Epoch 22/50\n",
      "1266/1266 [==============================] - 324s 255ms/step - loss: 0.9116 - acc: 0.6625 - val_loss: nan - val_acc: 0.5823\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.08103\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00011744049843400718.\n",
      "Epoch 23/50\n",
      "1266/1266 [==============================] - 324s 256ms/step - loss: 0.8428 - acc: 0.6789 - val_loss: nan - val_acc: 0.5807\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.08103\n",
      "Epoch 24/50\n",
      "1266/1266 [==============================] - 320s 252ms/step - loss: 0.8272 - acc: 0.6916 - val_loss: nan - val_acc: 0.5831\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.08103\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 9.395239758305252e-05.\n",
      "Epoch 00024: early stopping\n",
      "\n",
      " epoch #1\n",
      "\n",
      "WARNING:tensorflow:tf.keras.mixed_precision.experimental.LossScaleOptimizer is deprecated. Please use tf.keras.mixed_precision.LossScaleOptimizer instead. Note that the non-experimental LossScaleOptimizer does not take a DynamicLossScale but instead takes the dynamic configuration directly in the constructor. For example:\n",
      "  opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt)\n",
      "\n",
      "Warning! Model path already exists.\n",
      "Epoch 1/50\n",
      "1266/1266 [==============================] - 378s 266ms/step - loss: 1.7200 - acc: 0.3669 - val_loss: 2.0198 - val_acc: 0.2745\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.01982, saving model to /app/_data/models/EffB7_cropped/EffB7_1.h5\n",
      "Epoch 2/50\n",
      "1266/1266 [==============================] - 320s 253ms/step - loss: 1.4739 - acc: 0.4374 - val_loss: 1.9558 - val_acc: 0.6092\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.01982 to 1.95583, saving model to /app/_data/models/EffB7_cropped/EffB7_1.h5\n",
      "Epoch 3/50\n",
      "1266/1266 [==============================] - 323s 255ms/step - loss: 1.4153 - acc: 0.4717 - val_loss: 1.0349 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.95583 to 1.03488, saving model to /app/_data/models/EffB7_cropped/EffB7_1.h5\n",
      "Epoch 4/50\n",
      "1266/1266 [==============================] - 324s 255ms/step - loss: 1.3711 - acc: 0.5240 - val_loss: 4.7835 - val_acc: 0.6021\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.03488\n",
      "Epoch 5/50\n",
      "1266/1266 [==============================] - 325s 257ms/step - loss: 1.3914 - acc: 0.5071 - val_loss: 1.4683 - val_acc: 0.5902\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.03488\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.000559999980032444.\n",
      "Epoch 6/50\n",
      "1266/1266 [==============================] - 326s 257ms/step - loss: 1.2784 - acc: 0.5148 - val_loss: 6.4332 - val_acc: 0.5356\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.03488\n",
      "Epoch 7/50\n",
      "1266/1266 [==============================] - 327s 258ms/step - loss: 1.2830 - acc: 0.5227 - val_loss: 3.3045 - val_acc: 0.6076\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.03488\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00044799996539950375.\n",
      "Epoch 8/50\n",
      "1266/1266 [==============================] - 326s 257ms/step - loss: 1.1980 - acc: 0.5638 - val_loss: 9.8856 - val_acc: 0.5831\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.03488\n",
      "Epoch 9/50\n",
      "1266/1266 [==============================] - 326s 257ms/step - loss: 1.2067 - acc: 0.5514 - val_loss: nan - val_acc: 0.5063\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.03488\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00035839998163282876.\n",
      "Epoch 10/50\n",
      "1266/1266 [==============================] - 328s 259ms/step - loss: 1.1407 - acc: 0.5739 - val_loss: nan - val_acc: 0.6021\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.03488\n",
      "Epoch 11/50\n",
      "1266/1266 [==============================] - 327s 258ms/step - loss: 1.1459 - acc: 0.5997 - val_loss: 2.3023 - val_acc: 0.4755\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.03488\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002867199946194887.\n",
      "Epoch 12/50\n",
      "1266/1266 [==============================] - 329s 260ms/step - loss: 1.0964 - acc: 0.5894 - val_loss: 4.4207 - val_acc: 0.5411\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.03488\n",
      "Epoch 13/50\n",
      "1266/1266 [==============================] - 328s 259ms/step - loss: 1.1444 - acc: 0.5816 - val_loss: 0.9901 - val_acc: 0.6290\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.03488 to 0.99014, saving model to /app/_data/models/EffB7_cropped/EffB7_1.h5\n",
      "Epoch 14/50\n",
      "1266/1266 [==============================] - 324s 255ms/step - loss: 1.1208 - acc: 0.5825 - val_loss: 0.9707 - val_acc: 0.6305\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.99014 to 0.97065, saving model to /app/_data/models/EffB7_cropped/EffB7_1.h5\n",
      "Epoch 15/50\n",
      "1266/1266 [==============================] - 329s 259ms/step - loss: 1.0272 - acc: 0.6148 - val_loss: 1.0079 - val_acc: 0.6392\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.97065\n",
      "Epoch 16/50\n",
      "1266/1266 [==============================] - 327s 258ms/step - loss: 1.0115 - acc: 0.6201 - val_loss: 1.1484 - val_acc: 0.6290\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.97065\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00022937599569559098.\n",
      "Epoch 17/50\n",
      "1266/1266 [==============================] - 326s 257ms/step - loss: 0.9814 - acc: 0.6382 - val_loss: 0.9986 - val_acc: 0.6266\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.97065\n",
      "Epoch 18/50\n",
      "1266/1266 [==============================] - 330s 260ms/step - loss: 0.9602 - acc: 0.6414 - val_loss: 0.9873 - val_acc: 0.6290\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.97065\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0001835007918998599.\n",
      "Epoch 19/50\n",
      "1266/1266 [==============================] - 329s 259ms/step - loss: 0.9320 - acc: 0.6528 - val_loss: 1.1788 - val_acc: 0.5744\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.97065\n",
      "Epoch 20/50\n",
      "1266/1266 [==============================] - 328s 258ms/step - loss: 0.8876 - acc: 0.6612 - val_loss: 1.2668 - val_acc: 0.6028\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.97065\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00014680062886327506.\n",
      "Epoch 21/50\n",
      "1266/1266 [==============================] - 334s 264ms/step - loss: 0.7660 - acc: 0.7094 - val_loss: 1.1555 - val_acc: 0.5965\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.97065\n",
      "Epoch 22/50\n",
      "1266/1266 [==============================] - 326s 257ms/step - loss: 0.7263 - acc: 0.7278 - val_loss: 1.2249 - val_acc: 0.6052\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.97065\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00011744049843400718.\n",
      "Epoch 23/50\n",
      "1266/1266 [==============================] - 329s 259ms/step - loss: 0.6339 - acc: 0.7601 - val_loss: 1.3495 - val_acc: 0.5973\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.97065\n",
      "Epoch 24/50\n",
      "1266/1266 [==============================] - 331s 261ms/step - loss: 0.5365 - acc: 0.7976 - val_loss: 1.5868 - val_acc: 0.5902\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.97065\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 9.395239758305252e-05.\n",
      "Epoch 25/50\n",
      "1266/1266 [==============================] - 328s 258ms/step - loss: 0.4654 - acc: 0.8219 - val_loss: 1.6726 - val_acc: 0.6013\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.97065\n",
      "Epoch 26/50\n",
      "1266/1266 [==============================] - 329s 260ms/step - loss: 0.4084 - acc: 0.8496 - val_loss: 1.7953 - val_acc: 0.5815\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.97065\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 7.51619169022888e-05.\n",
      "Epoch 00026: early stopping\n",
      "\n",
      " epoch #2\n",
      "\n",
      "WARNING:tensorflow:tf.keras.mixed_precision.experimental.LossScaleOptimizer is deprecated. Please use tf.keras.mixed_precision.LossScaleOptimizer instead. Note that the non-experimental LossScaleOptimizer does not take a DynamicLossScale but instead takes the dynamic configuration directly in the constructor. For example:\n",
      "  opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt)\n",
      "\n",
      "Warning! Model path already exists.\n",
      "Epoch 1/50\n",
      "1266/1266 [==============================] - 383s 270ms/step - loss: 1.8996 - acc: 0.3717 - val_loss: 1.8335 - val_acc: 0.4604\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.83352, saving model to /app/_data/models/EffB7_cropped/EffB7_2.h5\n",
      "Epoch 2/50\n",
      "1266/1266 [==============================] - 325s 256ms/step - loss: 1.5885 - acc: 0.3872 - val_loss: 1.4082 - val_acc: 0.2903\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.83352 to 1.40822, saving model to /app/_data/models/EffB7_cropped/EffB7_2.h5\n",
      "Epoch 3/50\n",
      "1266/1266 [==============================] - 325s 257ms/step - loss: 1.3542 - acc: 0.4141 - val_loss: nan - val_acc: 0.3196\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.40822\n",
      "Epoch 4/50\n",
      "1266/1266 [==============================] - 329s 260ms/step - loss: 1.2507 - acc: 0.4388 - val_loss: nan - val_acc: 0.3916\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.40822\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.000559999980032444.\n",
      "Epoch 5/50\n",
      "1266/1266 [==============================] - 328s 258ms/step - loss: 1.2174 - acc: 0.4828 - val_loss: nan - val_acc: 0.4422\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.40822\n",
      "Epoch 6/50\n",
      "1266/1266 [==============================] - 329s 260ms/step - loss: 1.2259 - acc: 0.4637 - val_loss: nan - val_acc: 0.4533\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.40822\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00044799996539950375.\n",
      "Epoch 7/50\n",
      "1266/1266 [==============================] - 332s 262ms/step - loss: 1.2326 - acc: 0.4558 - val_loss: 1.9671 - val_acc: 0.4454\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.40822\n",
      "Epoch 8/50\n",
      "1266/1266 [==============================] - 328s 259ms/step - loss: 1.2154 - acc: 0.4775 - val_loss: 3.3574 - val_acc: 0.4747\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.40822\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00035839998163282876.\n",
      "Epoch 9/50\n",
      "1266/1266 [==============================] - 326s 258ms/step - loss: 1.2231 - acc: 0.4753 - val_loss: 1.2122 - val_acc: 0.4747\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.40822 to 1.21218, saving model to /app/_data/models/EffB7_cropped/EffB7_2.h5\n",
      "Epoch 10/50\n",
      "1266/1266 [==============================] - 327s 258ms/step - loss: 1.2242 - acc: 0.4634 - val_loss: nan - val_acc: 0.4604\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.21218\n",
      "Epoch 11/50\n",
      "1266/1266 [==============================] - 330s 260ms/step - loss: 1.2334 - acc: 0.4550 - val_loss: 1.2133 - val_acc: 0.4747\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.21218\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002867199946194887.\n",
      "Epoch 12/50\n",
      "1266/1266 [==============================] - 329s 260ms/step - loss: 1.2115 - acc: 0.4831 - val_loss: 1.4092 - val_acc: 0.4676\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.21218\n",
      "Epoch 13/50\n",
      "1266/1266 [==============================] - 332s 262ms/step - loss: 1.1839 - acc: 0.4889 - val_loss: 1.0929 - val_acc: 0.5783\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.21218 to 1.09286, saving model to /app/_data/models/EffB7_cropped/EffB7_2.h5\n",
      "Epoch 14/50\n",
      "1266/1266 [==============================] - 329s 260ms/step - loss: 1.1161 - acc: 0.5505 - val_loss: 1.1559 - val_acc: 0.5847\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.09286\n",
      "Epoch 15/50\n",
      "1266/1266 [==============================] - 328s 259ms/step - loss: 1.0755 - acc: 0.5759 - val_loss: 1.0638 - val_acc: 0.5831\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.09286 to 1.06376, saving model to /app/_data/models/EffB7_cropped/EffB7_2.h5\n",
      "Epoch 16/50\n",
      "1266/1266 [==============================] - 326s 257ms/step - loss: 1.0624 - acc: 0.5902 - val_loss: 1.0134 - val_acc: 0.6163\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.06376 to 1.01336, saving model to /app/_data/models/EffB7_cropped/EffB7_2.h5\n",
      "Epoch 17/50\n",
      "1266/1266 [==============================] - 330s 261ms/step - loss: 1.0514 - acc: 0.5981 - val_loss: 1.1094 - val_acc: 0.5562\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.01336\n",
      "Epoch 18/50\n",
      "1266/1266 [==============================] - 332s 262ms/step - loss: 1.0379 - acc: 0.5961 - val_loss: 1.0048 - val_acc: 0.6210\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.01336 to 1.00483, saving model to /app/_data/models/EffB7_cropped/EffB7_2.h5\n",
      "Epoch 19/50\n",
      "1266/1266 [==============================] - 334s 263ms/step - loss: 1.0287 - acc: 0.6097 - val_loss: 0.9867 - val_acc: 0.6258\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.00483 to 0.98668, saving model to /app/_data/models/EffB7_cropped/EffB7_2.h5\n",
      "Epoch 20/50\n",
      "1266/1266 [==============================] - 332s 262ms/step - loss: 1.0149 - acc: 0.6164 - val_loss: 0.9604 - val_acc: 0.6392\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.98668 to 0.96038, saving model to /app/_data/models/EffB7_cropped/EffB7_2.h5\n",
      "Epoch 21/50\n",
      "1266/1266 [==============================] - 331s 261ms/step - loss: 1.0178 - acc: 0.6060 - val_loss: 0.9677 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.96038\n",
      "Epoch 22/50\n",
      "1266/1266 [==============================] - 336s 265ms/step - loss: 0.9796 - acc: 0.6344 - val_loss: nan - val_acc: 0.6147\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.96038\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00022937599569559098.\n",
      "Epoch 23/50\n",
      "1266/1266 [==============================] - 331s 261ms/step - loss: 0.9786 - acc: 0.6200 - val_loss: 0.9853 - val_acc: 0.6258\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.96038\n",
      "Epoch 24/50\n",
      "1266/1266 [==============================] - 332s 262ms/step - loss: 0.9427 - acc: 0.6409 - val_loss: 0.9717 - val_acc: 0.6408\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.96038\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0001835007918998599.\n",
      "Epoch 25/50\n",
      "1266/1266 [==============================] - 332s 262ms/step - loss: 0.8888 - acc: 0.6558 - val_loss: 1.0413 - val_acc: 0.6258\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.96038\n",
      "Epoch 26/50\n",
      "1266/1266 [==============================] - 330s 261ms/step - loss: 0.8234 - acc: 0.6895 - val_loss: 1.0846 - val_acc: 0.6052\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.96038\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.00014680062886327506.\n",
      "Epoch 27/50\n",
      "1266/1266 [==============================] - 328s 259ms/step - loss: 0.7170 - acc: 0.7242 - val_loss: 1.0637 - val_acc: 0.6305\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.96038\n",
      "Epoch 28/50\n",
      "1266/1266 [==============================] - 330s 260ms/step - loss: 0.6454 - acc: 0.7485 - val_loss: 1.2770 - val_acc: 0.5997\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.96038\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.00011744049843400718.\n",
      "Epoch 29/50\n",
      "1266/1266 [==============================] - 328s 258ms/step - loss: 0.5778 - acc: 0.7795 - val_loss: 1.3808 - val_acc: 0.6092\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.96038\n",
      "Epoch 30/50\n",
      "1266/1266 [==============================] - 326s 257ms/step - loss: 0.4992 - acc: 0.8083 - val_loss: 1.3925 - val_acc: 0.6005\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.96038\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.395239758305252e-05.\n",
      "Epoch 31/50\n",
      "1266/1266 [==============================] - 326s 257ms/step - loss: 0.4424 - acc: 0.8323 - val_loss: 1.6697 - val_acc: 0.5870\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.96038\n",
      "Epoch 32/50\n",
      "1266/1266 [==============================] - 327s 258ms/step - loss: 0.4202 - acc: 0.8377 - val_loss: 1.8160 - val_acc: 0.5941\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.96038\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 7.51619169022888e-05.\n",
      "Epoch 00032: early stopping\n",
      "\n",
      " epoch #3\n",
      "\n",
      "WARNING:tensorflow:tf.keras.mixed_precision.experimental.LossScaleOptimizer is deprecated. Please use tf.keras.mixed_precision.LossScaleOptimizer instead. Note that the non-experimental LossScaleOptimizer does not take a DynamicLossScale but instead takes the dynamic configuration directly in the constructor. For example:\n",
      "  opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt)\n",
      "\n",
      "Warning! Model path already exists.\n",
      "Epoch 1/50\n",
      "1266/1266 [==============================] - 379s 267ms/step - loss: 1.7395 - acc: 0.4250 - val_loss: 1.2568 - val_acc: 0.5222\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.25681, saving model to /app/_data/models/EffB7_cropped/EffB7_3.h5\n",
      "Epoch 2/50\n",
      "1266/1266 [==============================] - 327s 258ms/step - loss: 1.2088 - acc: 0.5039 - val_loss: 1.0428 - val_acc: 0.6076\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.25681 to 1.04280, saving model to /app/_data/models/EffB7_cropped/EffB7_3.h5\n",
      "Epoch 3/50\n",
      "1266/1266 [==============================] - 322s 255ms/step - loss: 1.1240 - acc: 0.5650 - val_loss: 1.4648 - val_acc: 0.5672\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.04280\n",
      "Epoch 4/50\n",
      "1266/1266 [==============================] - 328s 259ms/step - loss: 1.0545 - acc: 0.5917 - val_loss: 1.1054 - val_acc: 0.5672\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.04280\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.000559999980032444.\n",
      "Epoch 5/50\n",
      "1266/1266 [==============================] - 326s 257ms/step - loss: 1.0717 - acc: 0.5844 - val_loss: 2.2995 - val_acc: 0.2983\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.04280\n",
      "Epoch 6/50\n",
      "1266/1266 [==============================] - 326s 257ms/step - loss: 1.0860 - acc: 0.5755 - val_loss: 0.9591 - val_acc: 0.6329\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.04280 to 0.95911, saving model to /app/_data/models/EffB7_cropped/EffB7_3.h5\n",
      "Epoch 7/50\n",
      "1266/1266 [==============================] - 327s 258ms/step - loss: 1.0061 - acc: 0.6205 - val_loss: 1.0050 - val_acc: 0.6282\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.95911\n",
      "Epoch 8/50\n",
      "1266/1266 [==============================] - 327s 258ms/step - loss: 1.0021 - acc: 0.6153 - val_loss: 1.0199 - val_acc: 0.6092\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.95911\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00044799996539950375.\n",
      "Epoch 9/50\n",
      "1266/1266 [==============================] - 326s 257ms/step - loss: 0.9346 - acc: 0.6470 - val_loss: 1.0003 - val_acc: 0.6242\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.95911\n",
      "Epoch 10/50\n",
      "1266/1266 [==============================] - 328s 259ms/step - loss: 0.9315 - acc: 0.6533 - val_loss: 1.1400 - val_acc: 0.5799\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.95911\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00035839998163282876.\n",
      "Epoch 11/50\n",
      "1266/1266 [==============================] - 327s 258ms/step - loss: 0.8451 - acc: 0.6873 - val_loss: 1.0300 - val_acc: 0.6361\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.95911\n",
      "Epoch 12/50\n",
      "1266/1266 [==============================] - 328s 259ms/step - loss: 0.7166 - acc: 0.7369 - val_loss: 1.1496 - val_acc: 0.5981\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.95911\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0002867199946194887.\n",
      "Epoch 13/50\n",
      "1266/1266 [==============================] - 328s 259ms/step - loss: 0.6229 - acc: 0.7742 - val_loss: 1.2747 - val_acc: 0.6210\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.95911\n",
      "Epoch 14/50\n",
      "1266/1266 [==============================] - 326s 258ms/step - loss: 0.5252 - acc: 0.8048 - val_loss: 1.4934 - val_acc: 0.5696\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.95911\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00022937599569559098.\n",
      "Epoch 15/50\n",
      "1266/1266 [==============================] - 324s 256ms/step - loss: 0.4221 - acc: 0.8421 - val_loss: 1.4723 - val_acc: 0.6084\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.95911\n",
      "Epoch 16/50\n",
      "1266/1266 [==============================] - 326s 257ms/step - loss: 0.3926 - acc: 0.8562 - val_loss: 1.5781 - val_acc: 0.6005\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.95911\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0001835007918998599.\n",
      "Epoch 17/50\n",
      "1266/1266 [==============================] - 325s 256ms/step - loss: 0.2955 - acc: 0.8888 - val_loss: 1.8089 - val_acc: 0.5910\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.95911\n",
      "Epoch 18/50\n",
      "1266/1266 [==============================] - 327s 258ms/step - loss: 0.2740 - acc: 0.8965 - val_loss: 2.0801 - val_acc: 0.6013\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.95911\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00014680062886327506.\n",
      "Epoch 00018: early stopping\n",
      "\n",
      " epoch #4\n",
      "\n",
      "WARNING:tensorflow:tf.keras.mixed_precision.experimental.LossScaleOptimizer is deprecated. Please use tf.keras.mixed_precision.LossScaleOptimizer instead. Note that the non-experimental LossScaleOptimizer does not take a DynamicLossScale but instead takes the dynamic configuration directly in the constructor. For example:\n",
      "  opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt)\n",
      "\n",
      "Warning! Model path already exists.\n",
      "Epoch 1/50\n",
      "1267/1267 [==============================] - 380s 267ms/step - loss: 1.8364 - acc: 0.3856 - val_loss: 3.0034 - val_acc: 0.3972\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.00336, saving model to /app/_data/models/EffB7_cropped/EffB7_4.h5\n",
      "Epoch 2/50\n",
      "1267/1267 [==============================] - 322s 254ms/step - loss: 1.5412 - acc: 0.3902 - val_loss: 8.2363 - val_acc: 0.2579\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 3.00336\n",
      "Epoch 3/50\n",
      "1267/1267 [==============================] - 325s 256ms/step - loss: 1.5418 - acc: 0.4069 - val_loss: 1.7071 - val_acc: 0.4715\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.00336 to 1.70708, saving model to /app/_data/models/EffB7_cropped/EffB7_4.h5\n",
      "Epoch 4/50\n",
      "1267/1267 [==============================] - 323s 255ms/step - loss: 1.5625 - acc: 0.3826 - val_loss: 1.3692 - val_acc: 0.4739\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.70708 to 1.36923, saving model to /app/_data/models/EffB7_cropped/EffB7_4.h5\n",
      "Epoch 5/50\n",
      "1267/1267 [==============================] - 326s 257ms/step - loss: 1.5298 - acc: 0.3795 - val_loss: 6.2878 - val_acc: 0.4707\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.36923\n",
      "Epoch 6/50\n",
      "1267/1267 [==============================] - 327s 258ms/step - loss: 1.4563 - acc: 0.4015 - val_loss: 1.9319 - val_acc: 0.1843\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.36923\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.000559999980032444.\n",
      "Epoch 7/50\n",
      "1267/1267 [==============================] - 326s 257ms/step - loss: 1.4720 - acc: 0.3924 - val_loss: 21.7744 - val_acc: 0.3655\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.36923\n",
      "Epoch 8/50\n",
      "1267/1267 [==============================] - 325s 256ms/step - loss: 1.3868 - acc: 0.4177 - val_loss: 1.3278 - val_acc: 0.4778\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.36923 to 1.32783, saving model to /app/_data/models/EffB7_cropped/EffB7_4.h5\n",
      "Epoch 9/50\n",
      "1267/1267 [==============================] - 325s 256ms/step - loss: 1.4064 - acc: 0.4161 - val_loss: 1.7414 - val_acc: 0.4660\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.32783\n",
      "Epoch 10/50\n",
      "1267/1267 [==============================] - 326s 257ms/step - loss: 1.4140 - acc: 0.4172 - val_loss: 2.6268 - val_acc: 0.3979\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.32783\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00044799996539950375.\n",
      "Epoch 11/50\n",
      "1267/1267 [==============================] - 326s 257ms/step - loss: 1.3753 - acc: 0.4210 - val_loss: nan - val_acc: 0.4509\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.32783\n",
      "Epoch 12/50\n",
      "1267/1267 [==============================] - 327s 258ms/step - loss: 1.3399 - acc: 0.4191 - val_loss: 2.2917 - val_acc: 0.4763\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.32783\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00035839998163282876.\n",
      "Epoch 13/50\n",
      "1267/1267 [==============================] - 324s 255ms/step - loss: 1.3262 - acc: 0.4328 - val_loss: 1.3008 - val_acc: 0.4691\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.32783 to 1.30083, saving model to /app/_data/models/EffB7_cropped/EffB7_4.h5\n",
      "Epoch 14/50\n",
      "1267/1267 [==============================] - 324s 255ms/step - loss: 1.3452 - acc: 0.4173 - val_loss: 1.5587 - val_acc: 0.4691\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.30083\n",
      "Epoch 15/50\n",
      "1267/1267 [==============================] - 325s 256ms/step - loss: 1.3266 - acc: 0.4208 - val_loss: 1.4608 - val_acc: 0.3196\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.30083\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0002867199946194887.\n",
      "Epoch 16/50\n",
      "1267/1267 [==============================] - 324s 256ms/step - loss: 1.3092 - acc: 0.4400 - val_loss: 3.3196 - val_acc: 0.4359\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.30083\n",
      "Epoch 17/50\n",
      "1267/1267 [==============================] - 326s 257ms/step - loss: 1.2930 - acc: 0.4472 - val_loss: nan - val_acc: 0.4604\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.30083\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00022937599569559098.\n",
      "Epoch 18/50\n",
      "1267/1267 [==============================] - 325s 256ms/step - loss: 1.2495 - acc: 0.4699 - val_loss: nan - val_acc: 0.4913\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.30083\n",
      "Epoch 19/50\n",
      "1267/1267 [==============================] - 326s 257ms/step - loss: 1.2365 - acc: 0.4858 - val_loss: nan - val_acc: 0.5063\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.30083\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0001835007918998599.\n",
      "Epoch 20/50\n",
      "1267/1267 [==============================] - 327s 257ms/step - loss: 1.2293 - acc: 0.4956 - val_loss: nan - val_acc: 0.5396\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.30083\n",
      "Epoch 21/50\n",
      "1267/1267 [==============================] - 325s 256ms/step - loss: 1.1855 - acc: 0.5001 - val_loss: nan - val_acc: 0.4541\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.30083\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.00014680062886327506.\n",
      "Epoch 22/50\n",
      "1267/1267 [==============================] - 325s 256ms/step - loss: 1.1805 - acc: 0.5103 - val_loss: nan - val_acc: 0.4335\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.30083\n",
      "Epoch 23/50\n",
      "1267/1267 [==============================] - 327s 258ms/step - loss: 1.1833 - acc: 0.5210 - val_loss: nan - val_acc: 0.5269\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.30083\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00011744049843400718.\n",
      "Epoch 24/50\n",
      "1267/1267 [==============================] - 325s 256ms/step - loss: 1.1248 - acc: 0.5472 - val_loss: nan - val_acc: 0.5451\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.30083\n",
      "Epoch 25/50\n",
      "1267/1267 [==============================] - 324s 255ms/step - loss: 1.1344 - acc: 0.5377 - val_loss: nan - val_acc: 0.4881\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.30083\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.395239758305252e-05.\n",
      "Epoch 00025: early stopping\n"
     ]
    }
   ],
   "source": [
    "for n in range(0, 5):\n",
    "    print(\"\\n epoch #\" + str(n) + \"\\n\")\n",
    "    val = df.loc[val_ids[n]].sample(frac=1, random_state=base_config[\"EFFB7\"][\"SEED\"])\n",
    "    train = df.loc[train_ids[n]].sample(\n",
    "        frac=1, random_state=base_config[\"EFFB7\"][\"SEED\"]\n",
    "    )\n",
    "\n",
    "    gen_train = Generator(\n",
    "        df=train,\n",
    "        batch_size=base_config[\"EFFB7\"][\"BATCH_SIZE\"],\n",
    "        seed=base_config[\"EFFB7\"][\"SEED\"],\n",
    "        img_size=base_config[\"EFFB7\"][\"IMG_SIZE\"],\n",
    "        cache_img_path=\"/app/_data/crop_npy_600/\",\n",
    "        shuffle=True,\n",
    "        label_columns=[\n",
    "            \"Negative for Pneumonia\",\n",
    "            \"Typical Appearance\",\n",
    "            \"Indeterminate Appearance\",\n",
    "            \"Atypical Appearance\",\n",
    "        ],\n",
    "        augment_fn=Aug_No_transform,\n",
    "        crop=True,\n",
    "    )\n",
    "    gen_valid = Generator(\n",
    "        df=val,\n",
    "        batch_size=base_config[\"EFFB7\"][\"BATCH_SIZE\"],\n",
    "        seed=base_config[\"EFFB7\"][\"SEED\"],\n",
    "        img_size=base_config[\"EFFB7\"][\"IMG_SIZE\"],\n",
    "        cache_img_path=\"/app/_data/crop_npy_600/\",\n",
    "        shuffle=False,\n",
    "        label_columns=[\n",
    "            \"Negative for Pneumonia\",\n",
    "            \"Typical Appearance\",\n",
    "            \"Indeterminate Appearance\",\n",
    "            \"Atypical Appearance\",\n",
    "        ],\n",
    "        augment_fn=None,\n",
    "        crop=True,\n",
    "    )\n",
    "\n",
    "    get_m = GetModel(\n",
    "        model_name=\"EFFB7\",\n",
    "        lr=0.0007,\n",
    "        activation_func=\"softmax\",\n",
    "        weights=base_config[\"EFFB7\"][\"WEIGHTS\"],\n",
    "        n_classes=4,\n",
    "        top_dropout_rate=None,\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"acc\"],\n",
    "    )\n",
    "    model = get_m.get_model()\n",
    "\n",
    "    callbacks = get_m.make_callback(\n",
    "        model_path=\"/app/_data/models/EffB7_cropped/\",\n",
    "        model_name=\"EffB7_\" + str(n) + \".h5\",\n",
    "        tensorboard_path=\"/app/.tensorboard/EffB7_cr00_\" + str(n),\n",
    "        patience_ES=12,\n",
    "        patience_RLR=2,\n",
    "        factor_LR=0.8,\n",
    "        metric_for_monitor=\"val_loss\",\n",
    "        metric_mode=\"min\",\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        gen_train,\n",
    "        validation_data=gen_valid,\n",
    "        epochs=50,\n",
    "        steps_per_epoch=len(train) // base_config[\"EFFB7\"][\"BATCH_SIZE\"],\n",
    "        validation_steps=len(val) // base_config[\"EFFB7\"][\"BATCH_SIZE\"],\n",
    "        verbose=1,\n",
    "        workers=20,\n",
    "        max_queue_size=500,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6211239-a8cb-4bff-8f1d-1bc229360b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
